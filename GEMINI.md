# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Task Master AI Instructions
**Import Task Master's development workflow commands and guidelines, treat as if import is in the main CLAUDE.md file.**
@./.taskmaster/CLAUDE.md

## Project Overview

Este √© um projeto de estudo sobre LLM as Judge - um sistema que compara e avalia respostas de diferentes modelos de linguagem. O projeto utiliza LangGraph para criar workflows comparativos entre LLMs.

## Architecture

### Core Components

- **laaj.config**: Configura√ß√£o centralizada de modelos e API keys
- **laaj.agents**: Factory functions para criar inst√¢ncias de LLMs e chains  
- **laaj.workflow**: LangGraph workflow para compara√ß√£o de LLMs
- **laaj.api**: FastAPI REST API com endpoints para compara√ß√£o via HTTP
- **laaj.langsmith_integration**: Cliente simplificado para tracing e observabilidade

### LLM Comparison Workflow

O workflow principal (`src/laaj/workflow/workflow.py`) implementa:

1. **Parallel LLM Execution**: Dois n√≥s executam em paralelo (llm_1, llm_2)
2. **Judge Evaluation**: Um terceiro LLM avalia qual resposta √© melhor
3. **State Management**: BestResponseState gerencia o estado do workflow

### API Architecture

REST API (`src/laaj/api/`) com estrutura modular:
- **main.py**: Aplica√ß√£o FastAPI principal com CORS e logging
- **routers/**: Endpoints organizados por funcionalidade (compare, models, health)
- **schemas/**: Pydantic models para request/response (em desenvolvimento)
- **core/**: Utilit√°rios compartilhados (em desenvolvimento)

### Supported Models

Configurados em `laaj.config.LITERAL_MODELS`:
- claude-4-sonnet (Anthropic)
- google-gemini-2.5-pro (Google)

Todos os modelos s√£o acessados via OpenRouter API (openrouter.ai/api/v1) usando ChatOpenAI interface.

## Development Commands

### Environment Setup
```bash
# Instalar depend√™ncias
uv sync

# Ativar ambiente virtual
uv shell

# Executar com uv
uv run <script>
```

### Running Code
```bash
# Executar workflow principal
uv run src/laaj/workflow/workflow.py

# Testar agentes individuais
uv run src/laaj/agents/agents.py

# Testar LLMs individuais
uv run src/laaj/agents/llms.py

# Executar API server (desenvolvimento)
uv run uvicorn laaj.api.main:app --reload --port 8000

# Executar API server (produ√ß√£o)  
uv run src/laaj/api/main.py

# Executar Streamlit app (se dispon√≠vel)
uv run streamlit run src/laaj/app.py

# Teste de tracing
uv run scripts/test_tracing.py
```

### Development Tools
```bash
# Formata√ß√£o de c√≥digo
uv run black src/

# Linting
uv run ruff src/

# Testes (quando dispon√≠veis)
uv run pytest tests/

# Instalar depend√™ncias de desenvolvimento
uv sync --extra dev
```

## Environment Variables Required

Criar arquivo `.env` na raiz:
```bash
OPENROUTER_API_KEY=your_openrouter_key  # Required - para acessar todos os modelos
LANGSMITH_API_KEY=your_langsmith_key    # Optional, para tracing e observabilidade  
LANGSMITH_PROJECT_NAME=llm-as-judge-study  # Optional, nome do projeto no LangSmith
LANGSMITH_ENDPOINT=https://api.smith.langchain.com  # Optional, endpoint customizado
```

## Project Philosophy

Segue abordagem **Just-in-Time Directory Creation**:
- Diret√≥rios criados apenas quando necess√°rios
- Estrutura evolui organicamente conforme necessidades
- Evita estruturas vazias ou placeholder
- Development usando Task Master AI para gest√£o de tarefas

## Key Patterns

### LLM Factory Pattern
```python
from laaj.agents import get_llm_anthropic_claude_4_sonnet, get_llm_google_gemini_pro
llm = get_llm_anthropic_claude_4_sonnet()
llm2 = get_llm_google_gemini_pro()
# Judge LLM ser√° configurado quando implementado
```

### Chain Creation Pattern
```python
from laaj.agents import chain_story, chain_laaj
story_chain = chain_story(llm)  # Para gerar hist√≥rias
judge_chain = chain_laaj(llm)   # Para avaliar respostas (usa prompt do LangSmith)
```

### Workflow State Management
- Use `BestResponseState` TypedDict para type safety
- Return dicion√°rios diretamente dos n√≥s (n√£o usar Command)
- State √© atualizado via merge autom√°tico do LangGraph
- Nodes: `llm_1`, `llm_2` (paralelos) ‚Üí `judge` (avalia√ß√£o)

### Judge System
- Judge usa prompt "laaj-prompt" do LangSmith
- Retorna `Preference` com valor 1 (modelo A) ou 2 (modelo B)
- Em caso de empate ou erro, retorna "Empate" ou mensagem de erro

### LangSmith Integration
Tracing autom√°tico habilitado quando LANGSMITH_API_KEY est√° dispon√≠vel:
```python
from laaj.langsmith_integration import LangSmithClient
client = LangSmithClient()  # Auto-configura tracing
```

### FastAPI Structure
```python
# API endpoints dispon√≠veis:
GET  /                           # Info da API
POST /api/v1/compare/            # Comparar LLMs (implementa√ß√£o pendente)
GET  /api/v1/models/             # Listar modelos dispon√≠veis
GET  /api/v1/models/{model_name} # Info de modelo espec√≠fico
GET  /api/v1/health/             # Health check b√°sico
GET  /api/v1/health/detailed     # Health check detalhado
GET  /docs                       # Swagger UI
```

### API Development Patterns
- Routers modulares em `src/laaj/api/routers/`
- CORS habilitado para desenvolvimento  
- Logging estruturado em cada endpoint
- Pydantic models para valida√ß√£o de request/response
- Endpoint `/docs` sempre dispon√≠vel para testing

## Current Development Status

### Completed
- ‚úÖ Configura√ß√£o base do projeto com uv
- ‚úÖ Estrutura modular da API FastAPI 
- ‚úÖ CORS middleware configurado
- ‚úÖ Logging estruturado em todos os endpoints
- ‚úÖ Endpoints b√°sicos: health, models, compare (skeleton)

### Next Priority
- üîÑ Integra√ß√£o do LangGraph workflow com API endpoints
- üîÑ Implementa√ß√£o completa do endpoint `/api/v1/compare/`
- üîÑ Schemas Pydantic para request/response
- üîÑ Conectar workflow de compara√ß√£o existente com a API
