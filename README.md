# ğŸ¤– LLM as Judge - AI Model Comparison System

![Python](https://img.shields.io/badge/Python-3.13+-blue?style=flat-square&logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green?style=flat-square&logo=fastapi)
![Docker](https://img.shields.io/badge/Docker-Ready-blue?style=flat-square&logo=docker)
![Kubernetes](https://img.shields.io/badge/Kubernetes-Native-blue?style=flat-square&logo=kubernetes)
![License](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)
![Platform](https://img.shields.io/badge/Platform-Linux%20ARM64%20%7C%20AMD64-lightgrey?style=flat-square)

Sistema avanÃ§ado de **comparaÃ§Ã£o e avaliaÃ§Ã£o de modelos de linguagem** usando arquitetura Judge com suporte a mÃºltiplos provedores de LLM. Focado na **comparaÃ§Ã£o de respostas prÃ©-geradas** com deployment production-ready em Docker e Kubernetes.

## ğŸ“– O que faz este sistema

O **LLM as Judge** Ã© uma soluÃ§Ã£o completa para **avaliaÃ§Ã£o objetiva de modelos de IA**:

## â­ CaracterÃ­sticas Principais

### ğŸ¯ InteligÃªncia Artificial AvanÃ§ada
- **Sistema Judge Multi-Modelo** com suporte dinÃ¢mico via testes automatizados
- **ComparaÃ§Ã£o Individual e em Lote** com processamento paralelo otimizado
- **Parsing Inteligente** com fallback robusto para interpretaÃ§Ã£o de respostas
- **EstatÃ­sticas de Performance** dos modelos em tempo real

### ğŸ›¡ï¸ Arquitetura Production-Ready
- **FastAPI** com documentaÃ§Ã£o OpenAPI automÃ¡tica
- **Docker Multi-Arch** (ARM64 + x64) com imagens otimizadas
- **Kubernetes Nativo** com Helm Charts e HPA
- **Health Checks AvanÃ§ados** com monitoramento detalhado

### ğŸ’» Tecnologias Utilizadas
- **Backend**: FastAPI, Python 3.13, Uvicorn, Pydantic
- **AI/ML**: OpenRouter API, LangSmith, LangChain, Multiple LLM Providers  
- **Deploy**: Docker, Kubernetes, Helm Charts
- **Observability**: Structured logging, LangSmith tracing, Health checks
- **Testing**: Pytest, Model compatibility tests, Stress testing

### ğŸš€ Funcionalidades
- âš–ï¸ **ComparaÃ§Ã£o de Respostas** usando modelos judge especializados
- ğŸ“Š **Processamento em Lote** com atÃ© 5 comparaÃ§Ãµes paralelas
- ğŸ”„ **Multi-Provider Support** via OpenRouter API
- ğŸ“ˆ **EstatÃ­sticas AutomÃ¡ticas** de vitÃ³rias/empates/erros por modelo
- ğŸ› ï¸ **Deploy Automatizado** com script Python integrado
- ğŸ” **Tracing Completo** via LangSmith para debugging

## ğŸ¤” O que sÃ£o LLM Judges

**LLM as Judge** Ã© uma metodologia onde um modelo de linguagem especializado avalia e compara respostas de outros LLMs de forma objetiva. Principais caracterÃ­sticas:

- ğŸ¯ **AvaliaÃ§Ã£o Objetiva** - Remove viÃ©s humano na comparaÃ§Ã£o
- âš¡ **Escalabilidade** - Processa milhares de comparaÃ§Ãµes rapidamente
- ğŸ† **Ranking AutomÃ¡tico** - Identifica automaticamente o melhor modelo
- ğŸ“‹ **CritÃ©rios Consistentes** - Aplica os mesmos padrÃµes de qualidade
- ğŸ”„ **Reprodutibilidade** - Resultados consistentes entre execuÃ§Ãµes

**Casos de uso:**
- Benchmark de modelos para seleÃ§Ã£o em produÃ§Ã£o
- A/B testing de diferentes versÃµes de LLMs
- AvaliaÃ§Ã£o de fine-tuning e prompt engineering
- Quality assurance em sistemas de IA

## ğŸ“ Estrutura do Projeto

```
llm-as-judge-study/
â”œâ”€â”€ README.md                           # Este arquivo de documentaÃ§Ã£o
â”œâ”€â”€ src/laaj/                          # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ api/                           # FastAPI REST API
â”‚   â”‚   â”œâ”€â”€ main.py                    # AplicaÃ§Ã£o principal
â”‚   â”‚   â”œâ”€â”€ routers/                   # Endpoints da API
â”‚   â”‚   â””â”€â”€ schemas/                   # Modelos Pydantic
â”‚   â”œâ”€â”€ agents/                        # Factory de LLMs e chains
â”‚   â”œâ”€â”€ config/                        # ConfiguraÃ§Ã£o centralizada
â”‚   â””â”€â”€ workflow/                      # Engine de comparaÃ§Ã£o
â”œâ”€â”€ deploy/                            # ConfiguraÃ§Ãµes de deployment
â”‚   â”œâ”€â”€ docker/                        # Docker e Docker Compose
â”‚   â”‚   â”œâ”€â”€ README.md                  # Guia de deploy Docker
â”‚   â”‚   â”œâ”€â”€ Dockerfile                 # Imagem ARM64
â”‚   â”‚   â”œâ”€â”€ Dockerfile-x64             # Imagem x64
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml         # Compose ARM64
â”‚   â”‚   â””â”€â”€ docker-compose-x64.yml     # Compose x64
â”‚   â”œâ”€â”€ helm/                          # Helm Charts para K8s
â”‚   â”‚   â””â”€â”€ llm-judge-api/             # Chart completo
â”‚   â”‚       â”œâ”€â”€ README.md              # Guia de deploy Kubernetes
â”‚   â”‚       â”œâ”€â”€ values.yaml            # ConfiguraÃ§Ã£o padrÃ£o
â”‚   â”‚       â”œâ”€â”€ values-dev.yaml        # Ambiente desenvolvimento
â”‚   â”‚       â”œâ”€â”€ values-prod.yaml       # Ambiente produÃ§Ã£o
â”‚   â”‚       â””â”€â”€ templates/             # Templates Kubernetes
â”‚   â””â”€â”€ troubleshooting.md             # Guia de troubleshooting
â”œâ”€â”€ deploy_helm.py                     # Script automatizado de deploy
â”œâ”€â”€ tests/                             # Testes automatizados
â””â”€â”€ requests.ipynb                     # Notebook de testes interativos
```

## ğŸš€ Como Usar - InÃ­cio RÃ¡pido

### PrÃ©-requisitos

- Python 3.13+ ou Docker
- Chave da API OpenRouter (obrigatÃ³ria)
- Chaves LangSmith, Anthropic, Mistral (opcionais)

### Setup Local com uv

```bash
# 1. Clonar repositÃ³rio
git clone https://github.com/cadugevaerd/llm-as-ajudge-study.git
cd llm-as-ajudge-study/

# 2. Configurar ambiente
cp .env.example .env
# Editar .env com suas API keys

# 3. Instalar dependÃªncias
uv sync

# 4. Executar API
uv run uvicorn laaj.api.main:app --reload --port 8000
```

### Teste RÃ¡pido da API

```bash
# Health check
curl http://localhost:8000/api/v1/health/

# Listar modelos disponÃ­veis
curl http://localhost:8000/api/v1/models/

# ComparaÃ§Ã£o individual
curl -X POST http://localhost:8000/api/v1/compare/ \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Qual a capital do Brasil?",
    "response_a": "A capital do Brasil Ã© BrasÃ­lia.",
    "response_b": "BrasÃ­lia Ã© a capital do Brasil desde 1960."
  }'

# ComparaÃ§Ã£o em lote com estatÃ­sticas
curl -X POST http://localhost:8000/api/v1/compare/batch \
  -H "Content-Type: application/json" \
  -d '{
    "comparisons": [
      {
        "input": "Pergunta 1",
        "response_a": "Resposta A1",
        "response_b": "Resposta B1"
      },
      {
        "input": "Pergunta 2", 
        "response_a": "Resposta A2",
        "response_b": "Resposta B2"
      }
    ]
  }'
```

### Deploy com Docker

```bash
# ARM64 (Apple Silicon)
docker-compose up -d

# x64 (Intel/AMD)
docker-compose -f docker-compose-x64.yml up -d

# Acesse: http://localhost:8000/docs
```

### Deploy em Kubernetes

```bash
# Deploy automatizado
python deploy_helm.py

# Deploy customizado
python deploy_helm.py --environment production --namespace ai-systems

# Monitoramento
kubectl get pods -l app.kubernetes.io/name=llm-judge-api
kubectl logs -f deployment/llm-judge-api
```

## ğŸ“š Funcionalidades Principais

### âš–ï¸ Sistema Judge Inteligente
- **Prompt Otimizado** via LangSmith Hub (langchain-ai/pairwise-evaluation-2)
- **MÃºltiplos Modelos Judge** com fallback automÃ¡tico
- **Parsing Robusto** com interpretaÃ§Ã£o de JSON + texto natural
- **Timeout ConfigurÃ¡vel** para controle de performance

### ğŸ“Š API REST Completa
- **Swagger UI** interativo em `/docs`
- **ComparaÃ§Ã£o Individual** com detalhes do raciocÃ­nio
- **ComparaÃ§Ã£o em Lote** com estatÃ­sticas agregadas
- **Health Checks** bÃ¡sicos e detalhados
- **Gerenciamento de Modelos** com status em tempo real

### ğŸ”„ Processamento Otimizado
- **ConcorrÃªncia Controlada** para evitar rate limits
- **Batch Processing** com atÃ© 5 comparaÃ§Ãµes paralelas
- **Error Handling** robusto com retry automÃ¡tico
- **Performance Metrics** integradas nas respostas

### ğŸ›¡ï¸ Deploy Production-Ready
- **Multi-Arch Support** (ARM64 + x64)
- **Security Best Practices** com non-root containers
- **Health Checks** automÃ¡ticos
- **Resource Limits** configurÃ¡veis
- **Graceful Shutdown** para zero downtime
- **Auto Scaling** com HPA no Kubernetes

## âš ï¸ ConfiguraÃ§Ãµes Importantes

### VariÃ¡veis de Ambiente ObrigatÃ³rias
```bash
# ObrigatÃ³rio - OpenRouter para modelos LLM
OPENROUTER_API_KEY=your_openrouter_key_here

# Opcional mas recomendado - LangSmith para tracing
LANGSMITH_API_KEY=your_langsmith_key_here
LANGSMITH_PROJECT_NAME=llm-as-judge-study

# Opcional - Modelos especÃ­ficos
ANTHROPIC_API_KEY=your_anthropic_key_here  
MISTRAL_API_KEY=your_mistral_key_here

# ConfiguraÃ§Ã£o da aplicaÃ§Ã£o
WORKFLOW_TIMEOUT_SECONDS=120
LOG_LEVEL=INFO
```

### Limites e RecomendaÃ§Ãµes
- **Batch mÃ¡ximo**: 5 comparaÃ§Ãµes por request
- **Timeout padrÃ£o**: 30s individual, 90s batch
- **Rate limits**: Respeitados automaticamente via OpenRouter
- **Modelos testados**: Gerados dinamicamente via testes de compatibilidade

## ğŸ“– DocumentaÃ§Ã£o Detalhada

Para informaÃ§Ãµes completas sobre deployment e troubleshooting:

ğŸ“„ **[Deploy com Docker](deploy/docker/README.md)**  
ğŸ“„ **[Deploy com Kubernetes](deploy/helm/llm-judge-api/README.md)**  
ğŸ“„ **[Guia de Troubleshooting](deploy/troubleshooting.md)**

## ğŸ§ª Exemplos de Uso

### ComparaÃ§Ã£o de Respostas de Chatbots
```python
# Avaliar qualidade de diferentes chatbots
response_data = {
    "input": "Como configurar SSL em nginx?",
    "response_a": "Use certbot com Let's Encrypt...",
    "response_b": "Configure manualmente os certificados..."
}
```

### Benchmark de Fine-tuning
```python
# Comparar modelo base vs fine-tuned
comparisons = [
    {
        "input": "Explique machine learning",
        "response_a": modelo_base_response,
        "response_b": modelo_fine_tuned_response
    }
    # ... mais comparaÃ§Ãµes
]
```

### A/B Testing de Prompts
```python
# Testar diferentes estratÃ©gias de prompt
for pergunta in dataset:
    resultado = comparar_respostas(
        pergunta,
        prompt_strategy_a(pergunta),
        prompt_strategy_b(pergunta)
    )
```

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

### Stack TecnolÃ³gico
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Frontend                        â”‚
â”‚   Swagger UI  â”‚  ReDoc  â”‚  Curl/Postman        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                FastAPI Layer                    â”‚
â”‚  Compare  â”‚  Models  â”‚  Health  â”‚  Batch        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Workflow Engine                    â”‚
â”‚    Judge Node  â”‚  State Management              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               LLM Providers                     â”‚
â”‚  OpenRouter â”‚ Anthropic â”‚ Mistral â”‚ LangSmith   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de ComparaÃ§Ã£o
1. **RecepÃ§Ã£o** - API recebe pergunta + duas respostas
2. **ValidaÃ§Ã£o** - Pydantic valida entrada e formata dados
3. **Judge** - Modelo LLM avalia qual resposta Ã© melhor
4. **Parsing** - Sistema extrai resultado (A/B/Empate) de forma robusta
5. **Resposta** - JSON estruturado com resultado e reasoning
6. **Tracing** - LangSmith registra toda a operaÃ§Ã£o (se configurado)

### ğŸ”¬ Sistema de Testes de Modelos

Os modelos suportados sÃ£o **determinados automaticamente** atravÃ©s do sistema de testes `tests/test_judge_models.py`:

#### **Processo Automatizado de SeleÃ§Ã£o**
1. **Rodada 1**: Teste de compatibilidade com pergunta de complexidade mÃ©dia
2. **Rodada 2**: Apenas modelos aprovados testam pergunta complexa
3. **GeraÃ§Ã£o de Config**: Modelos finalistas sÃ£o incluÃ­dos em `models_config.json`

#### **CritÃ©rios de AprovaÃ§Ã£o**
- âœ… **JSON estruturado vÃ¡lido** com campo `Preference`
- âœ… **Tempo de resposta < 5 segundos**
- âœ… **ConsistÃªncia de votaÃ§Ã£o** entre as rodadas
- âœ… **Conformidade com formato esperado**

#### **Modelos Testados Atualmente**
```python
# Lista atual em tests/test_judge_models.py (linha 25-45)
MODELS_TO_TEST = [
    "claude-sonnet-4-0",
    "claude-3-5-haiku-latest", 
    "google/gemini-2.5-pro",
    "google/gemini-2.5-flash",
    "openai/gpt-5",
    "openai/gpt-5-mini",
    "deepseek/deepseek-chat-v3.1",
    "mistral-large-latest",
    "x-ai/grok-4",
    "moonshotai/kimi-k2",
    # ... mais modelos testados automaticamente
]
```

#### **Executar Testes de Compatibilidade**
```bash
# Executar teste completo de duas rodadas
uv run tests/test_judge_models.py

# Resultados salvos em:
# - judge_models_two_rounds_results.json (detalhado)
# - src/laaj/config/models_config.json (config gerada)
```

#### **RelatÃ³rio AutomÃ¡tico**
O sistema gera relatÃ³rios detalhados incluindo:
- ğŸ† **Ranking de velocidade** dos modelos finalistas
- ğŸ“Š **AnÃ¡lise de votaÃ§Ã£o** e consistÃªncia
- âš–ï¸ **RecomendaÃ§Ã£o final** baseada em performance
- ğŸ”§ **ConfiguraÃ§Ã£o automÃ¡tica** para produÃ§Ã£o

**Vantagem**: Modelos sÃ£o validados continuamente e a configuraÃ§Ã£o Ã© atualizada automaticamente conforme novos modelos ficam disponÃ­veis.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Para contribuir:

1. **Fork** este repositÃ³rio
2. **Crie uma branch** para sua feature (`git checkout -b feature/AmazingFeature`)
3. **Commit** suas mudanÃ§as (`git commit -m 'feat: Add some AmazingFeature'`)
4. **Push** para a branch (`git push origin feature/AmazingFeature`)  
5. **Abra um Pull Request**

### ğŸ“ Guidelines
- Siga as convenÃ§Ãµes de cÃ³digo Python (Black + Ruff)
- Adicione testes para novas funcionalidades
- Atualize documentaÃ§Ã£o quando necessÃ¡rio
- Mantenha commits semÃ¢nticos e descritivos

### ğŸ§ª Executando Testes
```bash
# Testes de compatibilidade de modelos
uv run tests/test_judge_models.py

# Testes unitÃ¡rios (quando disponÃ­veis)
uv run pytest tests/

# Testes de stress via notebook
jupyter notebook requests.ipynb
```

### ğŸ› Reportando Bugs
- Use as [Issues](../../issues) para reportar bugs
- Inclua detalhes do ambiente (Python, Docker, K8s)
- Anexe logs relevantes da API
- Descreva passos para reproduzir o problema

## ğŸ‘¨â€ğŸ’» Autor

**Carlos Araujo** - Engenheiro DevOps & AI/ML  
Especialista em sistemas de IA, automaÃ§Ã£o de infraestrutura e Kubernetes

- ğŸ¤– **Expertise**: LLMs, FastAPI, Docker, Kubernetes, AI/ML Operations
- ğŸ¯ **Foco**: Sistemas de IA production-ready e infraestrutura escalÃ¡vel
- ğŸ’¼ **GitHub**: [@cadugevaerd](https://github.com/cadugevaerd)

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## â­ Apoie o Projeto

Se este projeto foi Ãºtil para vocÃª, considere:
- â­ **Dar uma estrela** no repositÃ³rio
- ğŸ› **Reportar bugs** ou sugerir melhorias
- ğŸ¤ **Contribuir** com cÃ³digo ou documentaÃ§Ã£o
- ğŸ“¢ **Compartilhar** com outros desenvolvedores de IA
- ğŸ’¡ **Sugerir novos modelos** ou funcionalidades

## ğŸ”— Links Ãšteis

- ğŸ“Š **Demo interativa**: http://localhost:8000/docs (apÃ³s deploy)
- ğŸ”— **OpenRouter Platform**: https://openrouter.ai/
- ğŸ” **LangSmith Tracing**: https://smith.langchain.com/
- ğŸ“š **FastAPI Docs**: https://fastapi.tiangolo.com/
- âš“ **Docker Hub**: `llm-judge-api:latest`

---

<div align="center">

**Desenvolvido por [Carlos Araujo](https://github.com/cadugevaerd) - Engenheiro DevOps & AI/ML**

*"Automatize a comparaÃ§Ã£o, meÃ§a a qualidade, escale com confianÃ§a"* ğŸ¤–âš–ï¸

</div>