import asyncio
import logging
from fastapi import APIRouter, HTTPException
from fastapi.responses import ORJSONResponse
from laaj.api.schemas import CompareRequest, ComparisonResponse, BatchCompareRequest, BatchComparisonResponse, BatchComparisonResult
from laaj.workflow.workflow import main as workflow_main, batch_judge_processing

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/", response_model=ComparisonResponse, response_class=ORJSONResponse)
async def compare_models(request: CompareRequest):
    """
    Compara duas respostas pr√©-geradas usando um modelo judge.
    N√£o gera novas respostas - apenas avalia respostas j√° existentes.
    """
    logger.info(f"Received comparison request for pre-generated responses")
    logger.info(f"Input: {request.input[:100]}...")
    logger.info(f"Response A: {len(request.response_a)} chars")
    logger.info(f"Response B: {len(request.response_b)} chars")
    
    try:
        # Executar workflow simplificado (apenas judge)
        result = await workflow_main(
            input_question=request.input,
            response_a=request.response_a,
            response_b=request.response_b,
            model_a_name=request.model_a_name,
            model_b_name=request.model_b_name,
            timeout_seconds=30
        )
        
        logger.info(f"Comparison completed with result: {result['better_response']}")
        return ComparisonResponse(**result)
        
    except asyncio.TimeoutError:
        logger.error("Comparison timed out.")
        raise HTTPException(status_code=408, detail="Comparison timed out")
        
    except ValueError as e:
        logger.error(f"Validation error: {e}")
        raise HTTPException(status_code=422, detail=f"Validation error: {e}")
        
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An internal server error occurred: {e}")

@router.post("/batch", response_model=BatchComparisonResponse, response_class=ORJSONResponse)
async def compare_models_batch(request: BatchCompareRequest):
    """
    Compara uma lista de respostas pr√©-geradas usando um modelo judge em paralelo.
    Usa LangChain abatch() para processamento eficiente e n√£o gera novas respostas.
    """
    import time
    start_time = time.time()
    
    logger.info(f"üî• [API-BATCH] Recebida requisi√ß√£o batch com {len(request.comparisons)} compara√ß√µes")
    
    # Log das compara√ß√µes recebidas (primeira linha de cada para debugging)
    for i, comp in enumerate(request.comparisons):
        logger.info(f"üìù [API-BATCH] Compara√ß√£o {i+1}: {comp.input[:50]}... | A={len(comp.response_a)} chars | B={len(comp.response_b)} chars")
    
    try:
        # Aplicar timeout de 90 segundos para batch (mais que individual)
        async with asyncio.timeout(90):
            
            # Executar processamento batch usando workflow
            logger.info(f"üöÄ [API-BATCH] Iniciando processamento paralelo...")
            batch_results = await batch_judge_processing(request.comparisons)
            
            # Calcular estat√≠sticas de performance
            model_a_wins = 0
            model_b_wins = 0
            ties = 0
            errors = 0
            successful_count = 0
            
            for result in batch_results:
                if result.better_response.startswith("ERRO") or result.better_response.startswith("TIMEOUT"):
                    errors += 1
                else:
                    successful_count += 1
                    if result.better_response == "A":
                        model_a_wins += 1
                    elif result.better_response == "B":
                        model_b_wins += 1
                    elif result.better_response == "Empate":
                        ties += 1
                    else:
                        # Outras respostas tamb√©m s√£o consideradas erros
                        errors += 1
                        successful_count -= 1
            
            # Determinar melhor modelo geral
            if model_a_wins > model_b_wins:
                best_model = "A"
            elif model_b_wins > model_a_wins:
                best_model = "B"
            elif model_a_wins == model_b_wins and (model_a_wins > 0 or model_b_wins > 0):
                best_model = "Empate"
            else:
                best_model = "N/A"
            
            elapsed_time = time.time() - start_time
            logger.info(f"üèÅ [API-BATCH] Processamento conclu√≠do em {elapsed_time:.2f}s")
            logger.info(f"üìä [API-BATCH] Estat√≠sticas: A={model_a_wins} | B={model_b_wins} | Empates={ties} | Erros={errors} | Melhor={best_model}")
            
            # Response estruturado que bate com o schema
            return BatchComparisonResponse(
                results=batch_results,
                total_comparisons=len(request.comparisons),
                successful=successful_count,
                execution_time=elapsed_time,
                model_a_wins=model_a_wins,
                model_b_wins=model_b_wins,
                ties=ties,
                errors=errors,
                best_model=best_model
            )
            
    except asyncio.TimeoutError:
        elapsed_time = time.time() - start_time
        error_msg = f"Processamento batch excedeu timeout de 90s ap√≥s {elapsed_time:.2f}s"
        
        logger.error(f"‚è∞ [API-BATCH] TIMEOUT: {error_msg}")
        
        # Retornar resultados de timeout para todas as compara√ß√µes
        timeout_results = []
        for comparison in request.comparisons:
            timeout_results.append(BatchComparisonResult(
                input=comparison.input,
                response_a=comparison.response_a,
                response_b=comparison.response_b,
                model_a_name=comparison.model_a_name,
                model_b_name=comparison.model_b_name,
                better_response=f"TIMEOUT - Excedeu 90s",
                judge_reasoning=f"O processamento batch foi interrompido por timeout ap√≥s {elapsed_time:.2f}s"
            ))
        
        return BatchComparisonResponse(
            results=timeout_results,
            total_comparisons=len(request.comparisons),
            successful=0,
            execution_time=elapsed_time,
            model_a_wins=0,
            model_b_wins=0,
            ties=0,
            errors=len(request.comparisons),
            best_model="N/A"
        )
        
    except ValueError as e:
        elapsed_time = time.time() - start_time
        logger.error(f"‚ùå [API-BATCH] Erro de valida√ß√£o: {str(e)}")
        
        raise HTTPException(status_code=422, detail=f"Validation error in batch processing: {e}")
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        error_type = type(e).__name__
        logger.error(f"‚ùå [API-BATCH] Erro inesperado ({error_type}): {str(e)}", exc_info=True)
        
        # Retornar resultados de erro para todas as compara√ß√µes
        error_results = []
        for comparison in request.comparisons:
            error_results.append(BatchComparisonResult(
                input=comparison.input,
                response_a=comparison.response_a,
                response_b=comparison.response_b,
                model_a_name=comparison.model_a_name,
                model_b_name=comparison.model_b_name,
                better_response=f"ERRO - {error_type}",
                judge_reasoning=f"Falha inesperada durante processamento batch: {str(e)}"
            ))
        
        return BatchComparisonResponse(
            results=error_results,
            total_comparisons=len(request.comparisons),
            successful=0,
            execution_time=elapsed_time,
            model_a_wins=0,
            model_b_wins=0,
            ties=0,
            errors=len(request.comparisons),
            best_model="N/A"
        )