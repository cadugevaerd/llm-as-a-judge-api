"""
Factory Pattern para cria√ß√£o de inst√¢ncias LLM

O Factory Pattern √© um padr√£o de design que fornece uma interface para criar 
objetos sem especificar suas classes concretas. Em vez de instanciar objetos 
diretamente no c√≥digo, delegamos essa responsabilidade para uma "f√°brica".

Vantagens:
- Centraliza a l√≥gica de cria√ß√£o em um s√≥ lugar
- Facilita a adi√ß√£o de novos modelos
- Reduz duplica√ß√£o de c√≥digo
- Torna o c√≥digo mais test√°vel e manuten√≠vel
"""

import logging
from typing import Callable, Dict, List
from langchain_openai import ChatOpenAI

from laaj.agents.llms import (
    get_llm_anthropic_claude_4_sonnet,
    get_llm_google_gemini_pro, 
    get_llm_gpt_5,
    get_llm_qwen_3_instruct,
    get_llm_deepseek,
    get_llm_llama_4_maverick
)
from laaj.config import LITERAL_MODELS

logger = logging.getLogger(__name__)


class LLMFactory:
    """
    Factory (F√°brica) para criar inst√¢ncias de Large Language Models (LLMs)
    
    Este padr√£o resolve o problema de ter m√∫ltiplos if/elif espalhados pelo c√≥digo
    para decidir qual modelo LLM instanciar. Em vez disso, centralizamos toda essa
    l√≥gica em uma √∫nica classe.
    
    Como funciona:
    1. Mantemos um dicion√°rio que mapeia nomes de modelos para suas fun√ß√µes criadoras
    2. Quando solicitado, consultamos o dicion√°rio e chamamos a fun√ß√£o apropriada
    3. Se o modelo n√£o existe, lan√ßamos uma exce√ß√£o informativa
    """
    
    # Dicion√°rio que mapeia nomes de modelos para suas fun√ß√µes de cria√ß√£o
    # Tipo: Dict[str, Callable[[], ChatOpenAI]]
    # - str: nome do modelo (ex: "claude-4-sonnet")  
    # - Callable[[], ChatOpenAI]: fun√ß√£o que n√£o recebe par√¢metros e retorna ChatOpenAI
    _models: Dict[str, Callable[[], ChatOpenAI]] = {
        # Modelo Llama 4 Maverick - Principal (melhor performance no teste)
        "llama-4-maverick": get_llm_llama_4_maverick,
        
        # Modelo Claude 4 Sonnet da Anthropic
        "claude-4-sonnet": get_llm_anthropic_claude_4_sonnet,
        
        # Modelo Google Gemini 2.5 Pro
        "google-gemini-2.5-pro": get_llm_google_gemini_pro,
        
        # Modelo GPT-5 (atrav√©s do OpenRouter)
        "gpt-5": get_llm_gpt_5,
        
        # Modelo Qwen 3 Instruct
        "qwen-3-instruct": get_llm_qwen_3_instruct,
        
        # Modelo DeepSeek
        "deepseek": get_llm_deepseek
    }
    
    @classmethod
    def create_llm(cls, model_name: str) -> ChatOpenAI:
        """
        M√©todo principal da Factory - cria uma inst√¢ncia do LLM solicitado
        
        Args:
            model_name (str): Nome do modelo a ser criado (ex: "claude-4-sonnet")
            
        Returns:
            ChatOpenAI: Inst√¢ncia configurada do modelo solicitado
            
        Raises:
            ValueError: Se o modelo solicitado n√£o estiver dispon√≠vel
            
        Exemplo:
            llm = LLMFactory.create_llm("claude-4-sonnet")
        """
        
        # Valida√ß√£o: verifica se o modelo existe no nosso registro
        if model_name not in cls._models:
            # Se n√£o existe, cria uma lista dos modelos dispon√≠veis para ajudar o usu√°rio
            available_models = ", ".join(cls._models.keys())
            error_msg = f"Modelo '{model_name}' n√£o encontrado. Dispon√≠veis: {available_models}"
            
            logger.error(f"‚ùå [FACTORY] {error_msg}")
            raise ValueError(error_msg)
        
        # Log informativo sobre qual modelo est√° sendo criado
        logger.info(f"üè≠ [FACTORY] Criando inst√¢ncia do modelo: {model_name}")
        
        # Busca a fun√ß√£o criadora no dicion√°rio e a executa
        # cls._models[model_name] retorna a fun√ß√£o (ex: get_llm_anthropic_claude_4_sonnet)
        # () no final executa a fun√ß√£o, retornando a inst√¢ncia do ChatOpenAI
        model_instance = cls._models[model_name]()
        
        logger.info(f"‚úÖ [FACTORY] Modelo {model_name} criado com sucesso")
        return model_instance
    
    @classmethod
    def get_available_models(cls) -> List[str]:
        """
        Retorna lista de todos os modelos dispon√≠veis na factory
        
        √ötil para:
        - Valida√ß√µes
        - Documenta√ß√£o din√¢mica  
        - Interfaces de usu√°rio
        - Testes
        
        Returns:
            List[str]: Lista com nomes de todos os modelos dispon√≠veis
            
        Exemplo:
            models = LLMFactory.get_available_models()
            # ['claude-4-sonnet', 'google-gemini-2.5-pro', 'gpt-5', 'qwen-3-instruct']
        """
        return list(cls._models.keys())
    
    @classmethod
    def is_model_supported(cls, model_name: str) -> bool:
        """
        Verifica se um modelo √© suportado sem tentar cri√°-lo
        
        √ötil para valida√ß√µes pr√©vias antes de chamar create_llm()
        
        Args:
            model_name (str): Nome do modelo a verificar
            
        Returns:
            bool: True se o modelo √© suportado, False caso contr√°rio
            
        Exemplo:
            if LLMFactory.is_model_supported("claude-4-sonnet"):
                llm = LLMFactory.create_llm("claude-4-sonnet")
        """
        return model_name in cls._models
    
    @classmethod
    def register_model(cls, model_name: str, creator_function: Callable[[], ChatOpenAI]) -> None:
        """
        Adiciona um novo modelo √† factory dinamicamente
        
        Permite extensibilidade - novos modelos podem ser adicionados em runtime
        
        Args:
            model_name (str): Nome identificador do modelo
            creator_function (Callable): Fun√ß√£o que cria inst√¢ncia do modelo
            
        Exemplo:
            def get_my_custom_llm():
                return ChatOpenAI(model="custom-model")
                
            LLMFactory.register_model("my-custom", get_my_custom_llm)
        """
        if model_name in cls._models:
            logger.warning(f"‚ö†Ô∏è [FACTORY] Sobrescrevendo modelo existente: {model_name}")
        
        cls._models[model_name] = creator_function
        logger.info(f"üìù [FACTORY] Modelo '{model_name}' registrado na factory")
    
    @classmethod
    def validate_config_models(cls) -> bool:
        """
        Valida se todos os modelos em LITERAL_MODELS est√£o dispon√≠veis na factory
        
        √ötil para verificar consist√™ncia entre configura√ß√£o e implementa√ß√£o
        
        Returns:
            bool: True se todos os modelos da config est√£o implementados
        """
        config_models = set(LITERAL_MODELS)  # Converte para set para compara√ß√£o
        factory_models = set(cls._models.keys())
        
        # Modelos que est√£o na config mas n√£o na factory
        missing_in_factory = config_models - factory_models
        
        # Modelos que est√£o na factory mas n√£o na config  
        extra_in_factory = factory_models - config_models
        
        if missing_in_factory:
            logger.error(f"‚ùå [FACTORY] Modelos na config mas n√£o na factory: {missing_in_factory}")
            
        if extra_in_factory:
            logger.warning(f"‚ö†Ô∏è [FACTORY] Modelos na factory mas n√£o na config: {extra_in_factory}")
            
        is_valid = len(missing_in_factory) == 0
        
        if is_valid:
            logger.info(f"‚úÖ [FACTORY] Valida√ß√£o OK - todos os modelos da config est√£o implementados")
        
        return is_valid


# Exemplo de uso e teste da factory
if __name__ == "__main__":
    """
    Exemplo de como usar a LLMFactory
    Este bloco s√≥ executa quando o arquivo √© rodado diretamente
    """
    
    # Configurar logging para ver os outputs
    logging.basicConfig(level=logging.INFO)
    
    print("üß™ Testando LLMFactory...")
    
    # 1. Listar modelos dispon√≠veis
    print(f"üìã Modelos dispon√≠veis: {LLMFactory.get_available_models()}")
    
    # 2. Verificar se modelo existe
    model_name = "claude-4-sonnet"
    if LLMFactory.is_model_supported(model_name):
        print(f"‚úÖ Modelo {model_name} √© suportado")
        
        # 3. Criar inst√¢ncia do modelo
        try:
            llm = LLMFactory.create_llm(model_name)
            print(f"üéâ Inst√¢ncia criada: {type(llm)}")
        except Exception as e:
            print(f"‚ùå Erro ao criar modelo: {e}")
    
    # 4. Testar modelo inexistente
    try:
        LLMFactory.create_llm("modelo-inexistente")
    except ValueError as e:
        print(f"‚úÖ Erro esperado capturado: {e}")
    
    # 5. Validar consist√™ncia com config
    LLMFactory.validate_config_models()
    
    print("üèÅ Teste conclu√≠do!")