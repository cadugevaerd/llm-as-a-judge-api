"""
Factory Pattern para criaÃ§Ã£o de instÃ¢ncias LLM com auto-descoberta dinÃ¢mica

O Factory Pattern implementado aqui utiliza auto-descoberta via JSON para:
- Carregar modelos dinamicamente sem hardcoding
- Suportar mÃºltiplos provedores de API automaticamente
- Permitir adiÃ§Ã£o de novos modelos apenas atualizando o JSON
- Manter compatibilidade com configuraÃ§Ãµes de fallback

Vantagens da abordagem dinÃ¢mica:
- Auto-descoberta de modelos via arquivo JSON gerado pelos testes
- Suporte automÃ¡tico a novos provedores (anthropic, openrouter, mistral, etc.)
- ConfiguraÃ§Ã£o flexÃ­vel per-modelo (tokens, timeout, parÃ¢metros especÃ­ficos)
- Sistema de fallback para quando JSON nÃ£o estÃ¡ disponÃ­vel
- Health checks e validaÃ§Ã£o automÃ¡tica
"""

import logging
from typing import Callable, Dict, List, Optional, Any
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from laaj.config.models_loader import models_loader, ModelsConfigError
from laaj.agents.llms import create_llm  # FunÃ§Ã£o genÃ©rica de criaÃ§Ã£o

logger = logging.getLogger(__name__)


class LLMFactory:
    """
    Factory (FÃ¡brica) para criar instÃ¢ncias de Large Language Models (LLMs) com auto-descoberta
    
    Esta implementaÃ§Ã£o revoluciona o padrÃ£o Factory tradicional:
    
    NOVO SISTEMA DINÃ‚MICO:
    1. Auto-descoberta de modelos via arquivo JSON gerado pelos testes de performance
    2. CriaÃ§Ã£o dinÃ¢mica de instÃ¢ncias usando configuraÃ§Ã£o per-modelo
    3. Suporte automÃ¡tico a mÃºltiplos provedores (anthropic, openrouter, mistral, etc.)
    4. Sistema de fallback para compatibilidade quando JSON nÃ£o disponÃ­vel
    
    VANTAGENS:
    - Sem hardcoding de modelos - tudo vem do JSON
    - AdiÃ§Ã£o de novos modelos apenas executando os testes
    - ConfiguraÃ§Ã£o flexÃ­vel per-modelo (tokens, timeout, parÃ¢metros)
    - Health checks automÃ¡ticos
    - Performance baseada em dados reais de testes
    
    Como funciona agora:
    1. Carrega configuraÃ§Ã£o JSON com modelos testados e aprovados
    2. Cria instÃ¢ncias dinamicamente usando configuraÃ§Ãµes especÃ­ficas
    3. Aplica fallback para modelos nÃ£o encontrados no JSON
    4. MantÃ©m compatibilidade com API existente
    """
    
    _cached_models: Dict[str, Callable[[], ChatOpenAI]] = {}
    _config_loaded = False
    
    @classmethod
    def _ensure_config_loaded(cls) -> None:
        """Garante que a configuraÃ§Ã£o JSON estÃ¡ carregada e os modelos registrados."""
        if not cls._config_loaded:
            cls._load_models_from_config()
            cls._config_loaded = True
    
    @classmethod
    def _load_models_from_config(cls) -> None:
        """Carrega modelos dinamicamente do arquivo JSON de configuraÃ§Ã£o."""
        try:
            logger.info("ğŸ”§ [FACTORY] Carregando modelos do arquivo JSON...")
            
            # Obter lista de modelos ativos da configuraÃ§Ã£o
            active_models = models_loader.get_active_models()
            
            if not active_models:
                logger.warning("âš ï¸ [FACTORY] Nenhum modelo ativo encontrado no JSON, usando fallback")
                cls._load_fallback_models()
                return
            
            # Carregar cada modelo ativo
            for model_id in active_models:
                model_config = models_loader.get_model_config(model_id)
                if model_config and model_config.status == 'active':
                    # Criar funÃ§Ã£o factory especÃ­fica para este modelo
                    creator_func = cls._create_model_factory_function(model_id, model_config)
                    cls._cached_models[model_id] = creator_func
                    logger.debug(f"âœ… [FACTORY] Modelo registrado: {model_id}")
            
            logger.info(f"âœ… [FACTORY] {len(cls._cached_models)} modelos carregados dinamicamente")
            
        except Exception as e:
            logger.error(f"âŒ [FACTORY] Erro ao carregar configuraÃ§Ã£o: {e}")
            logger.warning("âš ï¸ [FACTORY] Usando configuraÃ§Ã£o de fallback")
            cls._load_fallback_models()
    
    @classmethod 
    def _create_model_factory_function(cls, model_id: str, model_config) -> Callable[[], ChatOpenAI]:
        """
        Cria funÃ§Ã£o factory especÃ­fica para um modelo baseado na configuraÃ§Ã£o JSON.
        
        Args:
            model_id: ID do modelo
            model_config: ConfiguraÃ§Ã£o do modelo do JSON
            
        Returns:
            Callable: FunÃ§Ã£o que cria instÃ¢ncia do modelo
        """
        def create_model() -> ChatOpenAI:
            try:
                # Obter configuraÃ§Ã£o do provedor
                provider_config = models_loader.get_provider_config(model_config.provider)
                
                if not provider_config:
                    logger.error(f"âŒ [FACTORY] Provedor nÃ£o encontrado: {model_config.provider}")
                    raise ValueError(f"Provedor '{model_config.provider}' nÃ£o configurado")
                
                # Obter configuraÃ§Ãµes especÃ­ficas
                capabilities = model_config.capabilities or {}
                max_tokens = capabilities.get('max_tokens', 1024)
                temperature = capabilities.get('temperature', 0)
                timeout = capabilities.get('timeout', 30)
                
                logger.info(f"ğŸ­ [FACTORY] Criando {model_config.display_name} ({model_id})")
                
                # Criar instÃ¢ncia baseada no provedor
                if provider_config.api_type == 'anthropic':
                    # Usar ChatAnthropic diretamente para modelos Claude
                    import os
                    api_key = os.getenv(provider_config.requires_key)
                    if not api_key:
                        raise ValueError(f"API key nÃ£o encontrada: {provider_config.requires_key}")
                    
                    return ChatAnthropic(
                        model=model_id,
                        api_key=api_key,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        timeout=timeout
                    )
                else:
                    # Usar create_llm para modelos OpenRouter e outros
                    return create_llm(model_id)
                    
            except Exception as e:
                logger.error(f"âŒ [FACTORY] Erro ao criar {model_id}: {e}")
                raise
        
        return create_model
    
    @classmethod
    def _load_fallback_models(cls) -> None:
        """Carrega modelos de fallback quando JSON nÃ£o estÃ¡ disponÃ­vel."""
        logger.info("ğŸ”§ [FACTORY] Carregando modelos de fallback...")
        
        # Importar funÃ§Ãµes existentes para fallback
        try:
            from laaj.agents.llms import (
                get_llm_llama_4_maverick,
                get_llm_anthropic_claude_4_sonnet,
                get_llm_google_gemini_pro
            )
            
            fallback_models = {
                "llama-4-maverick": get_llm_llama_4_maverick,
                "claude-4-sonnet": get_llm_anthropic_claude_4_sonnet,
                "google-gemini-2.5-pro": get_llm_google_gemini_pro,
            }
            
            cls._cached_models.update(fallback_models)
            logger.info(f"âœ… [FACTORY] {len(fallback_models)} modelos de fallback carregados")
            
        except ImportError as e:
            logger.error(f"âŒ [FACTORY] Erro ao carregar funÃ§Ãµes de fallback: {e}")
            raise
    
    @classmethod
    def create_llm(cls, model_name: str) -> ChatOpenAI:
        """
        MÃ©todo principal da Factory - cria uma instÃ¢ncia do LLM solicitado dinamicamente
        
        NOVO: Agora usa auto-descoberta via JSON com fallback automÃ¡tico
        
        Args:
            model_name (str): Nome do modelo a ser criado (ex: "claude-4-sonnet")
            
        Returns:
            ChatOpenAI: InstÃ¢ncia configurada do modelo solicitado
            
        Raises:
            ValueError: Se o modelo solicitado nÃ£o estiver disponÃ­vel
            
        Exemplo:
            llm = LLMFactory.create_llm("claude-4-sonnet")
        """
        # Garantir que a configuraÃ§Ã£o estÃ¡ carregada
        cls._ensure_config_loaded()
        
        # Verificar se modelo estÃ¡ disponÃ­vel
        if model_name not in cls._cached_models:
            # Tentar atualizar configuraÃ§Ã£o antes de falhar
            try:
                logger.info(f"ğŸ”„ [FACTORY] Modelo '{model_name}' nÃ£o encontrado, recarregando configuraÃ§Ã£o...")
                cls._config_loaded = False
                cls._cached_models.clear()
                cls._ensure_config_loaded()
            except Exception as e:
                logger.warning(f"âš ï¸ [FACTORY] Erro ao recarregar configuraÃ§Ã£o: {e}")
        
        # ValidaÃ§Ã£o final
        if model_name not in cls._cached_models:
            available_models = ", ".join(cls._cached_models.keys())
            error_msg = f"Modelo '{model_name}' nÃ£o encontrado. DisponÃ­veis: {available_models}"
            
            logger.error(f"âŒ [FACTORY] {error_msg}")
            raise ValueError(error_msg)
        
        # Log informativo sobre qual modelo estÃ¡ sendo criado
        logger.info(f"ğŸ­ [FACTORY] Criando instÃ¢ncia do modelo: {model_name}")
        
        try:
            # Executar funÃ§Ã£o factory
            model_instance = cls._cached_models[model_name]()
            
            logger.info(f"âœ… [FACTORY] Modelo {model_name} criado com sucesso")
            return model_instance
            
        except Exception as e:
            logger.error(f"âŒ [FACTORY] Erro ao criar instÃ¢ncia de {model_name}: {e}")
            raise
    
    @classmethod
    def get_available_models(cls) -> List[str]:
        """
        Retorna lista de todos os modelos disponÃ­veis na factory (DINÃ‚MICO)
        
        NOVO: Carrega modelos dinamicamente do JSON de configuraÃ§Ã£o
        
        Ãštil para:
        - ValidaÃ§Ãµes
        - DocumentaÃ§Ã£o dinÃ¢mica  
        - Interfaces de usuÃ¡rio
        - Testes
        
        Returns:
            List[str]: Lista com nomes de todos os modelos disponÃ­veis
            
        Exemplo:
            models = LLMFactory.get_available_models()
            # Retorna modelos baseados nos testes de performance
        """
        cls._ensure_config_loaded()
        return list(cls._cached_models.keys())
    
    @classmethod
    def is_model_supported(cls, model_name: str) -> bool:
        """
        Verifica se um modelo Ã© suportado sem tentar criÃ¡-lo (DINÃ‚MICO)
        
        NOVO: Verifica tanto no cache quanto no JSON de configuraÃ§Ã£o
        
        Ãštil para validaÃ§Ãµes prÃ©vias antes de chamar create_llm()
        
        Args:
            model_name (str): Nome do modelo a verificar
            
        Returns:
            bool: True se o modelo Ã© suportado, False caso contrÃ¡rio
            
        Exemplo:
            if LLMFactory.is_model_supported("claude-4-sonnet"):
                llm = LLMFactory.create_llm("claude-4-sonnet")
        """
        cls._ensure_config_loaded()
        
        # Verificar cache primeiro
        if model_name in cls._cached_models:
            return True
            
        # Verificar se estÃ¡ disponÃ­vel no JSON mas nÃ£o carregado ainda
        return models_loader.is_model_available(model_name)
    
    @classmethod
    def register_model(cls, model_name: str, creator_function: Callable[[], ChatOpenAI]) -> None:
        """
        Adiciona um novo modelo Ã  factory dinamicamente
        
        NOVO: Registra no cache interno, nÃ£o modifica JSON
        
        Permite extensibilidade - novos modelos podem ser adicionados em runtime
        
        Args:
            model_name (str): Nome identificador do modelo
            creator_function (Callable): FunÃ§Ã£o que cria instÃ¢ncia do modelo
            
        Exemplo:
            def get_my_custom_llm():
                return ChatOpenAI(model="custom-model")
                
            LLMFactory.register_model("my-custom", get_my_custom_llm)
        """
        cls._ensure_config_loaded()
        
        if model_name in cls._cached_models:
            logger.warning(f"âš ï¸ [FACTORY] Sobrescrevendo modelo existente: {model_name}")
        
        cls._cached_models[model_name] = creator_function
        logger.info(f"ğŸ“ [FACTORY] Modelo '{model_name}' registrado na factory dinamicamente")
    
    @classmethod
    def get_default_model(cls) -> str:
        """
        ObtÃ©m o modelo padrÃ£o definido no JSON de configuraÃ§Ã£o.
        
        NOVO: Modelo padrÃ£o Ã© determinado pelos testes de performance
        
        Returns:
            str: Nome do modelo padrÃ£o (mais rÃ¡pido entre os testados)
        """
        try:
            return models_loader.get_default_model()
        except Exception as e:
            logger.warning(f"âš ï¸ [FACTORY] Erro ao obter modelo padrÃ£o: {e}")
            return "llama-4-maverick"  # fallback
    
    @classmethod
    def get_fastest_models(cls, limit: int = 5) -> List[Dict[str, Any]]:
        """
        ObtÃ©m os modelos mais rÃ¡pidos baseados nos testes de performance.
        
        NOVO: Dados reais de performance dos testes
        
        Args:
            limit: NÃºmero mÃ¡ximo de modelos a retornar
            
        Returns:
            List[Dict]: Lista de modelos ordenados por velocidade
        """
        try:
            return models_loader.get_fastest_models(limit)
        except Exception as e:
            logger.warning(f"âš ï¸ [FACTORY] Erro ao obter modelos mais rÃ¡pidos: {e}")
            return []
    
    @classmethod
    def get_models_by_provider(cls, provider: str) -> List[str]:
        """
        ObtÃ©m modelos de um provedor especÃ­fico.
        
        NOVO: Baseado na configuraÃ§Ã£o dinÃ¢mica de provedores
        
        Args:
            provider: Nome do provedor (anthropic, google, openai, etc.)
            
        Returns:
            List[str]: Lista de modelos do provedor
        """
        try:
            return models_loader.get_models_by_provider(provider)
        except Exception as e:
            logger.warning(f"âš ï¸ [FACTORY] Erro ao obter modelos do provedor {provider}: {e}")
            return []
    
    @classmethod
    def validate_json_config(cls) -> bool:
        """
        Valida se a configuraÃ§Ã£o JSON estÃ¡ consistente e disponÃ­vel.
        
        NOVO: Substitui validate_config_models para sistema dinÃ¢mico
        
        Ãštil para verificar consistÃªncia entre JSON e sistema
        
        Returns:
            bool: True se configuraÃ§Ã£o JSON estÃ¡ vÃ¡lida
        """
        try:
            health = models_loader.health_check()
            
            is_healthy = health['status'] in ['healthy', 'degraded']
            active_models = health.get('active_models', 0)
            
            if is_healthy and active_models > 0:
                logger.info(f"âœ… [FACTORY] ConfiguraÃ§Ã£o JSON vÃ¡lida - {active_models} modelos ativos")
                return True
            else:
                logger.warning(f"âš ï¸ [FACTORY] ConfiguraÃ§Ã£o JSON com problemas: {health}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ [FACTORY] Erro ao validar configuraÃ§Ã£o JSON: {e}")
            return False
    
    @classmethod
    def refresh_config(cls) -> bool:
        """
        ForÃ§a recarga da configuraÃ§Ã£o JSON e modelos.
        
        NOVO: Permite atualizaÃ§Ã£o dinÃ¢mica sem reiniciar aplicaÃ§Ã£o
        
        Returns:
            bool: True se recarregou com sucesso
        """
        try:
            # Limpar cache interno
            cls._cached_models.clear()
            cls._config_loaded = False
            
            # ForÃ§ar recarga do models_loader
            models_loader.refresh_config()
            
            # Recarregar modelos
            cls._ensure_config_loaded()
            
            logger.info("âœ… [FACTORY] ConfiguraÃ§Ã£o recarregada com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"âŒ [FACTORY] Erro ao recarregar configuraÃ§Ã£o: {e}")
            return False
    
    @classmethod
    def health_check(cls) -> Dict[str, Any]:
        """
        Verifica estado de saÃºde do sistema Factory + JSON.
        
        NOVO: Health check completo do sistema dinÃ¢mico
        
        Returns:
            Dict: RelatÃ³rio de saÃºde completo
        """
        try:
            cls._ensure_config_loaded()
            
            # Health check do models_loader
            loader_health = models_loader.health_check()
            
            # Health check do cache interno
            cached_models_count = len(cls._cached_models)
            config_loaded = cls._config_loaded
            
            # Testar criaÃ§Ã£o de um modelo (se disponÃ­vel)
            test_creation = False
            if cls._cached_models:
                try:
                    test_model = next(iter(cls._cached_models))
                    cls.create_llm(test_model)
                    test_creation = True
                except:
                    pass
            
            return {
                "factory_status": "healthy" if cached_models_count > 0 and config_loaded else "degraded",
                "cached_models_count": cached_models_count,
                "config_loaded": config_loaded,
                "test_model_creation": test_creation,
                "models_loader_health": loader_health,
                "available_models": list(cls._cached_models.keys())
            }
            
        except Exception as e:
            return {
                "factory_status": "error",
                "error": str(e),
                "cached_models_count": 0,
                "config_loaded": False
            }


# Exemplo de uso e teste da factory
if __name__ == "__main__":
    """
    Exemplo de como usar a LLMFactory
    Este bloco sÃ³ executa quando o arquivo Ã© rodado diretamente
    """
    
    # Configurar logging para ver os outputs
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ§ª Testando LLMFactory...")
    
    # 1. Listar modelos disponÃ­veis
    print(f"ğŸ“‹ Modelos disponÃ­veis: {LLMFactory.get_available_models()}")
    
    # 2. Verificar se modelo existe
    model_name = "claude-4-sonnet"
    if LLMFactory.is_model_supported(model_name):
        print(f"âœ… Modelo {model_name} Ã© suportado")
        
        # 3. Criar instÃ¢ncia do modelo
        try:
            llm = LLMFactory.create_llm(model_name)
            print(f"ğŸ‰ InstÃ¢ncia criada: {type(llm)}")
        except Exception as e:
            print(f"âŒ Erro ao criar modelo: {e}")
    
    # 4. Testar modelo inexistente
    try:
        LLMFactory.create_llm("modelo-inexistente")
    except ValueError as e:
        print(f"âœ… Erro esperado capturado: {e}")
    
    # 5. Validar configuraÃ§Ã£o JSON
    is_valid = LLMFactory.validate_json_config()
    print(f"ğŸ“Š ConfiguraÃ§Ã£o JSON vÃ¡lida: {is_valid}")
    
    # 6. Health check completo
    health = LLMFactory.health_check()
    print(f"ğŸ’Š Health check: {health['factory_status']}")
    
    # 7. Testar modelo padrÃ£o
    default_model = LLMFactory.get_default_model()
    print(f"ğŸ¯ Modelo padrÃ£o: {default_model}")
    
    # 8. Modelos mais rÃ¡pidos
    fastest = LLMFactory.get_fastest_models(3)
    if fastest:
        print(f"ğŸš€ Top 3 modelos mais rÃ¡pidos:")
        for i, model in enumerate(fastest, 1):
            print(f"   {i}. {model['display_name']}: {model['average_time']:.1f}s")
    
    print("ğŸ Teste concluÃ­do!")