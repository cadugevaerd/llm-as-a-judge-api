"""
Factory Pattern para cria√ß√£o de inst√¢ncias LLM com auto-descoberta din√¢mica

O Factory Pattern implementado aqui utiliza auto-descoberta via JSON para:
- Carregar modelos dinamicamente sem hardcoding
- Suportar m√∫ltiplos provedores de API automaticamente
- Permitir adi√ß√£o de novos modelos apenas atualizando o JSON
- Manter compatibilidade com configura√ß√µes de fallback

Vantagens da abordagem din√¢mica:
- Auto-descoberta de modelos via arquivo JSON gerado pelos testes
- Suporte autom√°tico a novos provedores (anthropic, openrouter, mistral, etc.)
- Configura√ß√£o flex√≠vel per-modelo (tokens, timeout, par√¢metros espec√≠ficos)
- Sistema de fallback para quando JSON n√£o est√° dispon√≠vel
- Health checks e valida√ß√£o autom√°tica
"""

import logging
from typing import Callable, Dict, List, Optional, Any
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from laaj.config.models_loader import models_loader, ModelsConfigError
from laaj.agents.llms import create_llm  # Fun√ß√£o gen√©rica de cria√ß√£o

logger = logging.getLogger(__name__)


class LLMFactory:
    """
    Factory (F√°brica) para criar inst√¢ncias de Large Language Models (LLMs) com auto-descoberta
    
    Esta implementa√ß√£o revoluciona o padr√£o Factory tradicional:
    
    NOVO SISTEMA DIN√ÇMICO:
    1. Auto-descoberta de modelos via arquivo JSON gerado pelos testes de performance
    2. Cria√ß√£o din√¢mica de inst√¢ncias usando configura√ß√£o per-modelo
    3. Suporte autom√°tico a m√∫ltiplos provedores (anthropic, openrouter, mistral, etc.)
    4. Sistema de fallback para compatibilidade quando JSON n√£o dispon√≠vel
    
    VANTAGENS:
    - Sem hardcoding de modelos - tudo vem do JSON
    - Adi√ß√£o de novos modelos apenas executando os testes
    - Configura√ß√£o flex√≠vel per-modelo (tokens, timeout, par√¢metros)
    - Health checks autom√°ticos
    - Performance baseada em dados reais de testes
    
    Como funciona agora:
    1. Carrega configura√ß√£o JSON com modelos testados e aprovados
    2. Cria inst√¢ncias dinamicamente usando configura√ß√µes espec√≠ficas
    3. Aplica fallback para modelos n√£o encontrados no JSON
    4. Mant√©m compatibilidade com API existente
    """
    
    _cached_models: Dict[str, Callable[[], ChatOpenAI]] = {}
    _config_loaded = False
    
    @classmethod
    def _ensure_config_loaded(cls) -> None:
        """Garante que a configura√ß√£o JSON est√° carregada e os modelos registrados."""
        if not cls._config_loaded:
            cls._load_models_from_config()
            cls._config_loaded = True
    
    @classmethod
    def _load_models_from_config(cls) -> None:
        """Carrega modelos dinamicamente do arquivo JSON de configura√ß√£o."""
        try:
            logger.info("üîß [FACTORY] Carregando modelos do arquivo JSON...")
            
            # Obter lista de modelos ativos da configura√ß√£o
            active_models = models_loader.get_active_models()
            
            if not active_models:
                logger.warning("‚ö†Ô∏è [FACTORY] Nenhum modelo ativo encontrado no JSON, usando fallback")
                cls._load_fallback_models()
                return
            
            # Carregar cada modelo ativo
            for model_id in active_models:
                model_config = models_loader.get_model_config(model_id)
                if model_config and model_config.status == 'active':
                    # Criar fun√ß√£o factory espec√≠fica para este modelo
                    creator_func = cls._create_model_factory_function(model_id, model_config)
                    cls._cached_models[model_id] = creator_func
                    logger.debug(f"‚úÖ [FACTORY] Modelo registrado: {model_id}")
            
            logger.info(f"‚úÖ [FACTORY] {len(cls._cached_models)} modelos carregados dinamicamente")
            
        except Exception as e:
            logger.error(f"‚ùå [FACTORY] Erro ao carregar configura√ß√£o: {e}")
            logger.warning("‚ö†Ô∏è [FACTORY] Usando configura√ß√£o de fallback")
            cls._load_fallback_models()
    
    @classmethod 
    def _create_model_factory_function(cls, model_id: str, model_config) -> Callable[[], ChatOpenAI]:
        """
        Cria fun√ß√£o factory espec√≠fica para um modelo baseado na configura√ß√£o JSON.
        
        Args:
            model_id: ID do modelo
            model_config: Configura√ß√£o do modelo do JSON
            
        Returns:
            Callable: Fun√ß√£o que cria inst√¢ncia do modelo
        """
        def create_model() -> ChatOpenAI:
            try:
                # Obter configura√ß√£o do provedor
                provider_config = models_loader.get_provider_config(model_config.provider)
                
                if not provider_config:
                    logger.error(f"‚ùå [FACTORY] Provedor n√£o encontrado: {model_config.provider}")
                    raise ValueError(f"Provedor '{model_config.provider}' n√£o configurado")
                
                # Obter configura√ß√µes espec√≠ficas
                capabilities = model_config.capabilities or {}
                max_tokens = capabilities.get('max_tokens', 1024)
                temperature = capabilities.get('temperature', 0)
                timeout = capabilities.get('timeout', 30)
                
                logger.info(f"üè≠ [FACTORY] Criando {model_config.display_name} ({model_id})")
                
                # Criar inst√¢ncia baseada no provedor
                if provider_config.api_type == 'anthropic':
                    # Usar ChatAnthropic diretamente para modelos Claude
                    import os
                    api_key = os.getenv(provider_config.requires_key)
                    if not api_key:
                        raise ValueError(f"API key n√£o encontrada: {provider_config.requires_key}")
                    
                    return ChatAnthropic(
                        model=model_id,
                        api_key=api_key,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        timeout=timeout
                    )
                else:
                    # Usar create_llm para modelos OpenRouter e outros
                    return create_llm(model_id)
                    
            except Exception as e:
                logger.error(f"‚ùå [FACTORY] Erro ao criar {model_id}: {e}")
                raise
        
        return create_model
    
    @classmethod
    def _load_fallback_models(cls) -> None:
        """Carrega modelos de fallback quando JSON n√£o est√° dispon√≠vel."""
        logger.info("üîß [FACTORY] Carregando modelos de fallback...")
        
        # Importar fun√ß√µes existentes para fallback
        try:
            from laaj.agents.llms import (
                get_llm_llama_4_maverick,
                get_llm_anthropic_claude_4_sonnet,
                get_llm_google_gemini_pro
            )
            
            fallback_models = {
                "llama-4-maverick": get_llm_llama_4_maverick,
                "claude-4-sonnet": get_llm_anthropic_claude_4_sonnet,
                "google-gemini-2.5-pro": get_llm_google_gemini_pro,
            }
            
            cls._cached_models.update(fallback_models)
            logger.info(f"‚úÖ [FACTORY] {len(fallback_models)} modelos de fallback carregados")
            
        except ImportError as e:
            logger.error(f"‚ùå [FACTORY] Erro ao carregar fun√ß√µes de fallback: {e}")
            raise
    
    @classmethod
    def create_llm(cls, model_name: str) -> ChatOpenAI:
        """
        M√©todo principal da Factory - cria uma inst√¢ncia do LLM solicitado dinamicamente
        
        NOVO: Agora usa auto-descoberta via JSON com fallback autom√°tico
        
        Args:
            model_name (str): Nome do modelo a ser criado (ex: "claude-4-sonnet")
            
        Returns:
            ChatOpenAI: Inst√¢ncia configurada do modelo solicitado
            
        Raises:
            ValueError: Se o modelo solicitado n√£o estiver dispon√≠vel
            
        Exemplo:
            llm = LLMFactory.create_llm("claude-4-sonnet")
        """
        # Garantir que a configura√ß√£o est√° carregada
        cls._ensure_config_loaded()
        
        # Verificar se modelo est√° dispon√≠vel
        if model_name not in cls._cached_models:
            # Tentar atualizar configura√ß√£o antes de falhar
            try:
                logger.info(f"üîÑ [FACTORY] Modelo '{model_name}' n√£o encontrado, recarregando configura√ß√£o...")
                cls._config_loaded = False
                cls._cached_models.clear()
                cls._ensure_config_loaded()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [FACTORY] Erro ao recarregar configura√ß√£o: {e}")
        
        # Valida√ß√£o final
        if model_name not in cls._cached_models:
            available_models = ", ".join(cls._cached_models.keys())
            error_msg = f"Modelo '{model_name}' n√£o encontrado. Dispon√≠veis: {available_models}"
            
            logger.error(f"‚ùå [FACTORY] {error_msg}")
            raise ValueError(error_msg)
        
        # Log informativo sobre qual modelo est√° sendo criado
        logger.info(f"üè≠ [FACTORY] Criando inst√¢ncia do modelo: {model_name}")
        
        try:
            # Executar fun√ß√£o factory
            model_instance = cls._cached_models[model_name]()
            
            logger.info(f"‚úÖ [FACTORY] Modelo {model_name} criado com sucesso")
            return model_instance
            
        except Exception as e:
            logger.error(f"‚ùå [FACTORY] Erro ao criar inst√¢ncia de {model_name}: {e}")
            raise
    
    @classmethod
    def get_available_models(cls) -> List[str]:
        """
        Retorna lista de todos os modelos dispon√≠veis na factory (DIN√ÇMICO)
        
        NOVO: Carrega modelos dinamicamente do JSON de configura√ß√£o
        
        √ötil para:
        - Valida√ß√µes
        - Documenta√ß√£o din√¢mica  
        - Interfaces de usu√°rio
        - Testes
        
        Returns:
            List[str]: Lista com nomes de todos os modelos dispon√≠veis
            
        Exemplo:
            models = LLMFactory.get_available_models()
            # Retorna modelos baseados nos testes de performance
        """
        cls._ensure_config_loaded()
        return list(cls._cached_models.keys())
    
    @classmethod
    def is_model_supported(cls, model_name: str) -> bool:
        """
        Verifica se um modelo √© suportado sem tentar cri√°-lo (DIN√ÇMICO)
        
        NOVO: Verifica tanto no cache quanto no JSON de configura√ß√£o
        
        √ötil para valida√ß√µes pr√©vias antes de chamar create_llm()
        
        Args:
            model_name (str): Nome do modelo a verificar
            
        Returns:
            bool: True se o modelo √© suportado, False caso contr√°rio
            
        Exemplo:
            if LLMFactory.is_model_supported("claude-4-sonnet"):
                llm = LLMFactory.create_llm("claude-4-sonnet")
        """
        cls._ensure_config_loaded()
        
        # Verificar cache primeiro
        if model_name in cls._cached_models:
            return True
            
        # Verificar se est√° dispon√≠vel no JSON mas n√£o carregado ainda
        return models_loader.is_model_available(model_name)
    
    @classmethod
    def register_model(cls, model_name: str, creator_function: Callable[[], ChatOpenAI]) -> None:
        """
        Adiciona um novo modelo √† factory dinamicamente
        
        NOVO: Registra no cache interno, n√£o modifica JSON
        
        Permite extensibilidade - novos modelos podem ser adicionados em runtime
        
        Args:
            model_name (str): Nome identificador do modelo
            creator_function (Callable): Fun√ß√£o que cria inst√¢ncia do modelo
            
        Exemplo:
            def get_my_custom_llm():
                return ChatOpenAI(model="custom-model")
                
            LLMFactory.register_model("my-custom", get_my_custom_llm)
        """
        cls._ensure_config_loaded()
        
        if model_name in cls._cached_models:
            logger.warning(f"‚ö†Ô∏è [FACTORY] Sobrescrevendo modelo existente: {model_name}")
        
        cls._cached_models[model_name] = creator_function
        logger.info(f"üìù [FACTORY] Modelo '{model_name}' registrado na factory dinamicamente")
    
    @classmethod
    def get_default_model(cls) -> str:
        """
        Obt√©m o modelo padr√£o definido no JSON de configura√ß√£o.
        
        NOVO: Modelo padr√£o √© determinado pelos testes de performance
        
        Returns:
            str: Nome do modelo padr√£o (mais r√°pido entre os testados)
        """
        try:
            return models_loader.get_default_model()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [FACTORY] Erro ao obter modelo padr√£o: {e}")
            return "llama-4-maverick"  # fallback
    
    @classmethod
    def get_fastest_models(cls, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Obt√©m os modelos mais r√°pidos baseados nos testes de performance.
        
        NOVO: Dados reais de performance dos testes
        
        Args:
            limit: N√∫mero m√°ximo de modelos a retornar
            
        Returns:
            List[Dict]: Lista de modelos ordenados por velocidade
        """
        try:
            return models_loader.get_fastest_models(limit)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [FACTORY] Erro ao obter modelos mais r√°pidos: {e}")
            return []
    
    @classmethod
    def get_models_by_provider(cls, provider: str) -> List[str]:
        """
        Obt√©m modelos de um provedor espec√≠fico.
        
        NOVO: Baseado na configura√ß√£o din√¢mica de provedores
        
        Args:
            provider: Nome do provedor (anthropic, google, openai, etc.)
            
        Returns:
            List[str]: Lista de modelos do provedor
        """
        try:
            return models_loader.get_models_by_provider(provider)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [FACTORY] Erro ao obter modelos do provedor {provider}: {e}")
            return []
    
    @classmethod
    def validate_json_config(cls) -> bool:
        """
        Valida se a configura√ß√£o JSON est√° consistente e dispon√≠vel.
        
        NOVO: Substitui validate_config_models para sistema din√¢mico
        
        √ötil para verificar consist√™ncia entre JSON e sistema
        
        Returns:
            bool: True se configura√ß√£o JSON est√° v√°lida
        """
        try:
            health = models_loader.health_check()
            
            is_healthy = health['status'] in ['healthy', 'degraded']
            active_models = health.get('active_models', 0)
            
            if is_healthy and active_models > 0:
                logger.info(f"‚úÖ [FACTORY] Configura√ß√£o JSON v√°lida - {active_models} modelos ativos")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è [FACTORY] Configura√ß√£o JSON com problemas: {health}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå [FACTORY] Erro ao validar configura√ß√£o JSON: {e}")
            return False
    
    @classmethod
    def refresh_config(cls) -> bool:
        """
        For√ßa recarga da configura√ß√£o JSON e modelos.
        
        NOVO: Permite atualiza√ß√£o din√¢mica sem reiniciar aplica√ß√£o
        
        Returns:
            bool: True se recarregou com sucesso
        """
        try:
            # Limpar cache interno
            cls._cached_models.clear()
            cls._config_loaded = False
            
            # For√ßar recarga do models_loader
            models_loader.refresh_config()
            
            # Recarregar modelos
            cls._ensure_config_loaded()
            
            logger.info("‚úÖ [FACTORY] Configura√ß√£o recarregada com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå [FACTORY] Erro ao recarregar configura√ß√£o: {e}")
            return False
    
    @classmethod
    def health_check(cls) -> Dict[str, Any]:
        """
        Verifica estado de sa√∫de do sistema Factory + JSON.
        
        NOVO: Health check completo do sistema din√¢mico
        
        Returns:
            Dict: Relat√≥rio de sa√∫de completo
        """
        try:
            cls._ensure_config_loaded()
            
            # Health check do models_loader
            loader_health = models_loader.health_check()
            
            # Health check do cache interno
            cached_models_count = len(cls._cached_models)
            config_loaded = cls._config_loaded
            
            # Testar cria√ß√£o de um modelo (se dispon√≠vel)
            test_creation = False
            if cls._cached_models:
                try:
                    test_model = next(iter(cls._cached_models))
                    cls.create_llm(test_model)
                    test_creation = True
                except:
                    pass
            
            return {
                "factory_status": "healthy" if cached_models_count > 0 and config_loaded else "degraded",
                "cached_models_count": cached_models_count,
                "config_loaded": config_loaded,
                "test_model_creation": test_creation,
                "models_loader_health": loader_health,
                "available_models": list(cls._cached_models.keys())
            }
            
        except Exception as e:
            return {
                "factory_status": "error",
                "error": str(e),
                "cached_models_count": 0,
                "config_loaded": False
            }


# Exemplo de uso e teste da factory
if __name__ == "__main__":
    """
    Exemplo de como usar a LLMFactory
    Este bloco s√≥ executa quando o arquivo √© rodado diretamente
    """
    
    # Configurar logging para ver os outputs
    logging.basicConfig(level=logging.INFO)
    
    print("üß™ Testando LLMFactory...")
    
    # 1. Listar modelos dispon√≠veis
    print(f"üìã Modelos dispon√≠veis: {LLMFactory.get_available_models()}")
    
    # 2. Verificar se modelo existe
    model_name = "claude-4-sonnet"
    if LLMFactory.is_model_supported(model_name):
        print(f"‚úÖ Modelo {model_name} √© suportado")
        
        # 3. Criar inst√¢ncia do modelo
        try:
            llm = LLMFactory.create_llm(model_name)
            print(f"üéâ Inst√¢ncia criada: {type(llm)}")
        except Exception as e:
            print(f"‚ùå Erro ao criar modelo: {e}")
    
    # 4. Testar modelo inexistente
    try:
        LLMFactory.create_llm("modelo-inexistente")
    except ValueError as e:
        print(f"‚úÖ Erro esperado capturado: {e}")
    
    # 5. Validar configura√ß√£o JSON
    is_valid = LLMFactory.validate_json_config()
    print(f"üìä Configura√ß√£o JSON v√°lida: {is_valid}")
    
    # 6. Health check completo
    health = LLMFactory.health_check()
    print(f"üíä Health check: {health['factory_status']}")
    
    # 7. Testar modelo padr√£o
    default_model = LLMFactory.get_default_model()
    print(f"üéØ Modelo padr√£o: {default_model}")
    
    # 8. Modelos mais r√°pidos
    fastest = LLMFactory.get_fastest_models(3)
    if fastest:
        print(f"üöÄ Top 3 modelos mais r√°pidos:")
        for i, model in enumerate(fastest, 1):
            print(f"   {i}. {model['display_name']}: {model['average_time']:.1f}s")
    
    print("üèÅ Teste conclu√≠do!")