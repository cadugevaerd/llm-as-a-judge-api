"""
Factory functions para cria√ß√£o din√¢mica de inst√¢ncias espec√≠ficas de LLMs.

SISTEMA REFATORADO PARA MULTI-PROVIDER:
- Suporte din√¢mico a anthropic, openrouter, google, mistral, xai, deepseek, etc.
- Configura√ß√£o espec√≠fica por provedor baseada no JSON
- Functions de cria√ß√£o geradas dinamicamente baseadas nos testes
- Compatibilidade mantida com fun√ß√µes existentes para fallback
- Configura√ß√µes otimizadas por modelo (tokens, timeout, par√¢metros especiais)

NOVO: Auto-descoberta via JSON + M√∫ltiplos Provedores + Configura√ß√£o Flex√≠vel
"""

import os
import logging
from typing import Union, Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_mistralai import ChatMistralAI

import laaj.config as config
from laaj.config.models_loader import models_loader

logger = logging.getLogger(__name__)


def create_llm(model_name: str, **override_params) -> Union[ChatOpenAI, ChatAnthropic, ChatMistralAI]:
    """
    Cria inst√¢ncia do modelo LLM pelo nome com suporte multi-provider din√¢mico.
    
    NOVO SISTEMA:
    - Auto-detec√ß√£o do provedor baseada no modelo
    - Configura√ß√µes espec√≠ficas por provedor do JSON
    - Par√¢metros otimizados por modelo dos testes
    - Fallback inteligente para modelos n√£o configurados
    
    Args:
        model_name (str): Nome do modelo a ser criado (ex: "claude-4-sonnet")
        **override_params: Par√¢metros para sobrescrever configura√ß√£o padr√£o
        
    Returns:
        Union[ChatOpenAI, ChatAnthropic, ...]: Inst√¢ncia configurada do modelo solicitado
        
    Examples:
        # Usando configura√ß√£o padr√£o do JSON
        llm = create_llm("claude-4-sonnet")
        
        # Sobrescrevendo par√¢metros
        llm = create_llm("google-gemini-2.5-pro", temperature=0.5, max_tokens=2048)
        
        # Modelo OpenRouter
        llm = create_llm("meta-llama/llama-4-maverick")
    """
    
    try:
        # Tentar obter configura√ß√£o do JSON primeiro
        model_config = models_loader.get_model_config(model_name)
        provider_config = None
        
        if model_config:
            provider_config = models_loader.get_provider_config(model_config.provider)
            logger.debug(f"üîß [LLMS] Usando configura√ß√£o JSON para {model_name}")
        else:
            # Fallback: detectar provedor pelo nome do modelo
            provider_type = _detect_provider_from_model_name(model_name)
            logger.warning(f"‚ö†Ô∏è [LLMS] Modelo {model_name} n√£o encontrado no JSON, usando fallback: {provider_type}")
        
        # Obter configura√ß√µes (JSON ou fallback)
        if model_config and provider_config:
            return _create_from_json_config(model_name, model_config, provider_config, **override_params)
        else:
            return _create_from_fallback(model_name, **override_params)
            
    except Exception as e:
        logger.error(f"‚ùå [LLMS] Erro ao criar modelo {model_name}: {e}")
        raise


def _create_from_json_config(
    model_name: str, 
    model_config, 
    provider_config, 
    **override_params
) -> Union[ChatOpenAI, ChatAnthropic, ChatMistralAI]:
    """
    Cria inst√¢ncia baseada na configura√ß√£o JSON com par√¢metros otimizados dos testes.
    
    ESTRAT√âGIA DE PROVEDORES:
    - Anthropic: Usa API oficial (claude-sonnet-4-0, claude-3-5-haiku-latest)
    - Mistral: Usa API oficial (mistral-large-latest, mistral-medium-latest, mistral-small-latest)
    - Todos os outros: OpenRouter como proxy (Google, OpenAI, XAI, DeepSeek, Qwen, etc.)
    
    Args:
        model_name: Nome do modelo
        model_config: Configura√ß√£o do modelo do JSON
        provider_config: Configura√ß√£o do provedor do JSON  
        **override_params: Par√¢metros para sobrescrever
        
    Returns:
        Inst√¢ncia configurada do LLM
    """
    
    # Obter configura√ß√µes do JSON (testadas e otimizadas)
    capabilities = model_config.capabilities or {}
    base_params = {
        'model': model_name,
        'max_tokens': capabilities.get('max_tokens', 1024),
        'temperature': capabilities.get('temperature', 0),
        'timeout': capabilities.get('timeout', 60)
    }
    
    # Aplicar overrides
    base_params.update(override_params)
    
    logger.info(f"üè≠ [LLMS] Criando {model_config.display_name} via {provider_config.api_type}")
    
    # Modelos que usam Anthropic diretamente
    anthropic_direct_models = [
        "claude-sonnet-4-0",
        "claude-3-5-haiku-latest"
    ]
    
    # Modelos que usam Mistral diretamente
    mistral_direct_models = [
        "mistral-large-latest",
        "mistral-medium-latest", 
        "mistral-small-latest"
    ]
    
    # ========== ANTHROPIC DIRETO ==========
    if model_name in anthropic_direct_models:
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            logger.warning(f"‚ö†Ô∏è [LLMS] ANTHROPIC_API_KEY n√£o encontrada para {model_name}, usando fallback OpenRouter")
            return _create_openrouter_fallback(model_name, **base_params)
        
        # Remover 'model' dos base_params para evitar duplica√ß√£o
        anthropic_params = {k: v for k, v in base_params.items() if k != 'model'}
        
        return ChatAnthropic(
            model=model_name,
            api_key=api_key,
            thinking={"type": "disabled"}, # Desabilita o Reasoning
            **anthropic_params
        )
    
    # ========== MISTRAL DIRETO ==========
    elif model_name in mistral_direct_models:
        api_key = os.getenv("MISTRAL_API_KEY")
        if not api_key:
            logger.warning(f"‚ö†Ô∏è [LLMS] MISTRAL_API_KEY n√£o encontrada para {model_name}, usando fallback OpenRouter")
            return _create_openrouter_fallback(model_name, **base_params)
        
        # Remover 'model' dos base_params para evitar duplica√ß√£o
        mistral_params = {k: v for k, v in base_params.items() if k != 'model'}
        
        return ChatMistralAI(
            model=model_name,
            mistral_api_key=api_key,
            **mistral_params
        )
    
    # ========== TODOS OS OUTROS VIA OPENROUTER ==========
    else:
        return _create_openrouter_fallback(model_name, **base_params)


def _create_openrouter_fallback(model_name: str, **params) -> ChatOpenAI:
    """
    Cria inst√¢ncia via OpenRouter (estrat√©gia padr√£o).
    
    Args:
        model_name: Nome do modelo
        **params: Par√¢metros base
        
    Returns:
        ChatOpenAI configurada para OpenRouter
    """
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise ValueError("OPENROUTER_API_KEY n√£o encontrada")
    
    # Configura√ß√µes especiais para OpenRouter baseadas nos testes
    extra_body = _get_openrouter_extra_body(model_name)
    
    # Remover 'model' dos params para evitar duplica√ß√£o
    openrouter_params = {k: v for k, v in params.items() if k != 'model'}
    
    return ChatOpenAI(
        model=model_name,
        api_key=api_key,
        base_url="https://openrouter.ai/api/v1",
        extra_body=extra_body,
        **openrouter_params
    )


def _create_from_fallback(model_name: str, **override_params) -> ChatOpenAI:
    """
    Cria inst√¢ncia usando configura√ß√£o de fallback (sistema legado).
    
    Args:
        model_name: Nome do modelo
        **override_params: Par√¢metros para sobrescrever
        
    Returns:
        Inst√¢ncia ChatOpenAI configurada
    """
    
    logger.warning(f"‚ö†Ô∏è [LLMS] Usando configura√ß√£o de fallback para {model_name}")
    
    # Configura√ß√µes hardcoded para modelos conhecidos (sistema legado)
    llms_can_disabled_reasoning = [
        "google/gemma-3-27b-it",
        "google/gemini-2.5-flash",
        "qwen/qwen3-235b-a22b-2507",
    ]
    
    anthropics_llms = [
        "anthropic/claude-sonnet-4",
        "claude-4-sonnet",
        "claude-3-5-haiku-latest"
    ]
    
    # Par√¢metros base
    base_params = {
        'model': model_name,
        'api_key': config.OPENROUTER_API,
        'base_url': "https://openrouter.ai/api/v1",
        'temperature': 0,
        'timeout': 30,
        'max_tokens': 2048 if model_name in anthropics_llms else 1024
    }
    
    # Extra body para OpenRouter
    extra_body = {
        "reasoning": {
            "enabled": False if model_name in llms_can_disabled_reasoning else True,
            "effort": "minimal" if model_name not in llms_can_disabled_reasoning and model_name not in anthropics_llms else None,
            "max_tokens": 1024 if model_name in anthropics_llms else None,
        }
    }
    
    base_params['extra_body'] = extra_body
    
    # Aplicar overrides
    base_params.update(override_params)
    
    return ChatOpenAI(**base_params)


def _detect_provider_from_model_name(model_name: str) -> str:
    """
    Detecta provedor baseado no nome do modelo (fallback).
    
    Args:
        model_name: Nome do modelo
        
    Returns:
        str: Nome do provedor detectado
    """
    
    if model_name.startswith("claude-") or model_name.startswith("anthropic/"):
        return "anthropic"
    elif model_name.startswith("google/") or model_name.startswith("gemini"):
        return "google"  
    elif model_name.startswith("openai/") or model_name.startswith("gpt-"):
        return "openai"
    elif model_name.startswith("mistral"):
        return "mistral"
    elif model_name.startswith("x-ai/") or model_name.startswith("grok"):
        return "xai"
    elif model_name.startswith("deepseek/"):
        return "deepseek"
    elif model_name.startswith("qwen/"):
        return "qwen"
    elif model_name.startswith("meta-llama/") or model_name.startswith("llama"):
        return "meta"
    else:
        return "openrouter"  # Padr√£o


def _get_openrouter_extra_body(model_name: str) -> Dict[str, Any]:
    """
    Obt√©m configura√ß√£o extra_body otimizada para OpenRouter baseada nos testes.
    
    Args:
        model_name: Nome do modelo
        
    Returns:
        Dict: Configura√ß√£o extra_body
    """
    
    # Modelos que podem desabilitar reasoning (baseado nos testes)
    can_disable_reasoning = [
        "google/gemma-3-27b-it",
        "google/gemini-2.5-flash",
        "google/gemini-2.5-flash-lite", 
        "qwen/qwen3-235b-a22b-2507",
    ]
    
    # Modelos Anthropic que precisam configura√ß√£o especial
    anthropic_models = [
        "anthropic/claude-sonnet-4",
        "claude-sonnet-4-0",
        "claude-3-5-haiku-latest"
    ]
    
    extra_body = {
        "reasoning": {
            "enabled": False if model_name in can_disable_reasoning else True,
        }
    }
    
    # Configura√ß√£o espec√≠fica para modelos n√£o-Anthropic
    if model_name not in anthropic_models and model_name not in can_disable_reasoning:
        extra_body["reasoning"]["effort"] = "minimal"
    
    # Configura√ß√£o espec√≠fica para modelos Anthropic  
    if model_name in anthropic_models:
        extra_body["reasoning"]["max_tokens"] = 1024
    
    return extra_body


# ========== FUN√á√ïES DE FALLBACK PARA COMPATIBILIDADE ==========
# Mantidas para compatibilidade com c√≥digo existente


def get_llm_llama_4_maverick() -> ChatOpenAI:
    """Cria inst√¢ncia do Llama 4 Maverick - modelo judge principal com melhor performance."""
    return create_llm("meta-llama/llama-4-maverick")


def get_llm_anthropic_claude_4_sonnet() -> Union[ChatAnthropic, ChatOpenAI]:
    """Cria inst√¢ncia do Claude 4 Sonnet - modelo judge principal."""
    try:
        # Tentar usar Anthropic diretamente se dispon√≠vel
        if config.ANTHROPIC_API:
            return ChatAnthropic(
                model="claude-3-5-sonnet-20241022",
                api_key=config.ANTHROPIC_API,
                max_tokens=1024,
                temperature=0,
                timeout=30
            )
        else:
            # Fallback para OpenRouter
            return create_llm("anthropic/claude-sonnet-4")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [LLMS] Erro ao criar Claude via Anthropic, usando OpenRouter: {e}")
        return create_llm("anthropic/claude-sonnet-4")


def get_llm_google_gemini_pro() -> ChatOpenAI:
    """Cria inst√¢ncia do Google Gemini 2.5 Pro - modelo judge alternativo."""
    return create_llm("google/gemini-2.5-pro")


def get_llm_gpt_5() -> ChatOpenAI:
    """Cria inst√¢ncia do GPT-5 - modelo judge alternativo."""
    return create_llm("openai/gpt-5")
    

def get_llm_qwen_3_instruct() -> ChatOpenAI:
    """Cria inst√¢ncia do Qwen 3 Instruct - modelo judge alternativo."""
    return create_llm("qwen/qwen3-30b-a3b-instruct-2507")
    

def get_llm_deepseek() -> ChatOpenAI:
    """Cria inst√¢ncia do DeepSeek - modelo judge alternativo."""
    return create_llm("deepseek/deepseek-chat-v3.1")


def get_llm_google_gemma() -> ChatOpenAI:
    """Cria inst√¢ncia do Google Gemma - modelo judge alternativo."""
    return create_llm("google/gemma-3-27b-it")


# ========== FUN√á√ïES DE CONVENI√äNCIA E UTILIT√ÅRIOS ==========


def get_available_providers() -> Dict[str, Dict[str, Any]]:
    """
    Obt√©m lista de provedores dispon√≠veis com suas configura√ß√µes.
    
    Returns:
        Dict: Dicion√°rio de provedores dispon√≠veis
    """
    try:
        config = models_loader.get_config()
        return config.get('providers', {})
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [LLMS] Erro ao obter provedores: {e}")
        return {}


def test_model_creation(model_name: str) -> bool:
    """
    Testa se √© poss√≠vel criar uma inst√¢ncia de um modelo.
    
    Args:
        model_name: Nome do modelo para testar
        
    Returns:
        bool: True se conseguiu criar, False caso contr√°rio
    """
    try:
        llm = create_llm(model_name)
        logger.info(f"‚úÖ [LLMS] Teste de cria√ß√£o bem-sucedido: {model_name}")
        return True
    except Exception as e:
        logger.error(f"‚ùå [LLMS] Falha no teste de cria√ß√£o para {model_name}: {e}")
        return False


def get_model_info(model_name: str) -> Optional[Dict[str, Any]]:
    """
    Obt√©m informa√ß√µes sobre um modelo espec√≠fico.
    
    Args:
        model_name: Nome do modelo
        
    Returns:
        Optional[Dict]: Informa√ß√µes do modelo ou None se n√£o encontrado
    """
    try:
        model_config = models_loader.get_model_config(model_name)
        if not model_config:
            return None
            
        provider_config = models_loader.get_provider_config(model_config.provider)
        
        return {
            "model_id": model_config.id,
            "display_name": model_config.display_name,
            "provider": model_config.provider,
            "provider_config": provider_config.__dict__ if provider_config else None,
            "is_default": model_config.is_default,
            "status": model_config.status,
            "performance": model_config.performance,
            "capabilities": model_config.capabilities
        }
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [LLMS] Erro ao obter informa√ß√µes do modelo {model_name}: {e}")
        return None


# ========== EXEMPLO DE USO E TESTE ==========


if __name__ == "__main__":
    """
    Exemplo de uso das fun√ß√µes din√¢micas multi-provider
    """
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("üß™ Testando sistema din√¢mico multi-provider...")
    
    # 1. Testar cria√ß√£o din√¢mica
    test_models = [
        "claude-4-sonnet",
        "google/gemini-2.5-pro", 
        "meta-llama/llama-4-maverick",
        "deepseek/deepseek-chat-v3.1"
    ]
    
    for model in test_models:
        try:
            print(f"\nüîß Testando: {model}")
            
            # Obter informa√ß√µes
            info = get_model_info(model)
            if info:
                print(f"   Display: {info['display_name']}")
                print(f"   Provider: {info['provider']}")
                print(f"   Status: {info['status']}")
            
            # Testar cria√ß√£o
            success = test_model_creation(model)
            print(f"   Cria√ß√£o: {'‚úÖ' if success else '‚ùå'}")
            
        except Exception as e:
            print(f"   ‚ùå Erro: {e}")
    
    # 2. Listar provedores dispon√≠veis
    print(f"\nüìã Provedores dispon√≠veis:")
    providers = get_available_providers()
    for provider_id, provider_info in providers.items():
        print(f"   ‚Ä¢ {provider_id}: {provider_info.get('api_type', 'unknown')}")
    
    print("\nüèÅ Teste conclu√≠do!")