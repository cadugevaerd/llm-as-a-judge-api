# PRD - LLM as Judge API MVP

## Overview
Desenvolver uma API REST para o sistema LLM as Judge que permite comparar respostas entre diferentes modelos de linguagem através de endpoints OpenAPI. A API deve permitir flexibilidade na entrada de dados, configuração dinâmica de modelos e fornecer resultados estruturados via JSON.

## Core Features

### 1. API REST com OpenAPI Specification
- **O que faz**: API RESTful documentada com OpenAPI/Swagger para comparação de LLMs
- **Por que é importante**: Permite integração fácil com outros sistemas e oferece documentação interativa
- **Como funciona**: Endpoints HTTP com validação automática de schema e documentação Swagger UI

### 2. Endpoint de Comparação Flexível  
- **O que faz**: Endpoint POST que aceita tanto respostas pré-geradas quanto geração em tempo real
- **Por que é importante**: Suporta diferentes casos de uso - análise de respostas existentes ou geração nova
- **Como funciona**: Payload JSON que especifica modelos, respostas opcionais e configuração do juiz

### 3. Configuração Dinâmica de Modelos
- **O que faz**: Permite especificar modelos LLM e juiz via parâmetros da requisição
- **Por que é importante**: Flexibilidade para testar diferentes combinações de modelos
- **Como funciona**: Validação de modelos disponíveis e instanciação dinâmica

### 4. Endpoints de Metadados
- **O que faz**: Endpoints para listar modelos disponíveis e status do sistema
- **Por que é importante**: Facilita integração e descoberta de recursos da API
- **Como funciona**: Endpoints GET para informações do sistema

## User Experience

### User Personas
- **Desenvolvedor de Sistema**: Integra a API em aplicações existentes
- **Pesquisador**: Usa a API via Postman/curl para experimentos
- **Analista de Dados**: Consome a API para análises em lote

### Key API Flows

#### Comparação com Geração em Tempo Real
```
POST /api/v1/compare
{
  "question": "Explique machine learning",
  "llm_a": "claude-4-sonnet",
  "llm_b": "google-gemini-2.5-pro", 
  "judge_model": "claude-4-sonnet"
}
```

#### Comparação com Respostas Pré-Geradas
```
POST /api/v1/compare
{
  "question": "Explique machine learning",
  "llm_a_name": "GPT-4",
  "llm_a_response": "Machine learning é uma subárea...",
  "llm_b_name": "Claude", 
  "llm_b_response": "Aprendizado de máquina...",
  "judge_model": "google-gemini-2.5-pro"
}
```

#### Descoberta de Recursos
```
GET /api/v1/models  # Lista modelos disponíveis
GET /api/v1/health  # Status da API
GET /docs          # Documentação Swagger UI
```

## Technical Architecture

### System Components
- **FastAPI Application**: Framework web assíncrono para API REST
- **Pydantic Models**: Validação de schema automática e documentação OpenAPI
- **LLM Router**: Roteamento inteligente para diferentes provedores
- **Async Workflow Engine**: LangGraph com suporte assíncrono
- **Response Formatter**: Serialização JSON padronizada

### API Schema (OpenAPI)
```python
# Modelos Pydantic para OpenAPI
class ComparisonRequest(BaseModel):
    question: str = Field(..., description="Pergunta para os LLMs")
    llm_a: Optional[str] = Field(None, description="Modelo A para geração")
    llm_b: Optional[str] = Field(None, description="Modelo B para geração")
    llm_a_name: Optional[str] = Field(None, description="Nome do agente A")
    llm_a_response: Optional[str] = Field(None, description="Resposta pré-gerada A")
    llm_b_name: Optional[str] = Field(None, description="Nome do agente B") 
    llm_b_response: Optional[str] = Field(None, description="Resposta pré-gerada B")
    judge_model: str = Field(..., description="Modelo usado como juiz")

class ComparisonResponse(BaseModel):
    id: str = Field(..., description="ID único da comparação")
    question: str
    response_a: str
    response_b: str
    llm_a_name: str
    llm_b_name: str
    judge_model: str
    winner: str = Field(..., description="Vencedor: 'llm_a', 'llm_b' ou 'tie'")
    confidence: Optional[float] = Field(None, description="Confiança do juiz (0-1)")
    reasoning: str = Field(..., description="Justificativa do juiz")
    execution_time: float = Field(..., description="Tempo de execução em segundos")
    timestamp: datetime

class ModelInfo(BaseModel):
    name: str
    provider: str
    available: bool
    description: Optional[str]

class HealthResponse(BaseModel):
    status: str
    version: str
    available_models: int
    uptime: float
```

### Endpoints Structure
```
/api/v1/
├── compare (POST)           # Comparação principal
├── models (GET)             # Lista modelos disponíveis  
├── health (GET)             # Status da aplicação
├── comparison/{id} (GET)    # Buscar comparação por ID
└── docs (GET)               # Documentação Swagger UI
```

### Infrastructure Requirements
- **Framework**: FastAPI com Uvicorn
- **Validação**: Pydantic v2 para schemas
- **Documentação**: OpenAPI 3.0 com Swagger UI
- **Storage**: Redis para cache opcional
- **Logging**: Estruturado com Python logging
- **Monitoring**: Health checks e métricas básicas

## Development Roadmap

### MVP Requirements (Fase Atual)

#### FastAPI Foundation
- Configurar estrutura base da API com FastAPI
- Implementar modelos Pydantic para request/response
- Configurar roteamento e middleware básico
- Integrar OpenAPI documentation

#### Core Endpoints
- Endpoint POST /api/v1/compare com validação completa
- Endpoint GET /api/v1/models para descoberta
- Endpoint GET /api/v1/health para monitoring
- Tratamento de erros padronizado

#### Integração LangGraph Assíncrona
- Adaptar workflow atual para execução assíncrona
- Implementar timeout e cancelamento de requisições
- Otimizar para múltiplas requisições concorrentes

#### Validação e Testes
- Validação de modelos disponíveis
- Testes de API com pytest-httpx
- Documentação interativa completa

### Future Enhancements (Pós-MVP)
- Autenticação e rate limiting
- Cache de respostas com Redis
- Endpoints de análise estatística
- Webhook notifications
- Batch processing endpoints
- WebSocket para streaming responses

## Logical Dependency Chain

### Foundation (Prioridade 1)
1. **Setup FastAPI Base**: Estrutura inicial da aplicação
2. **Pydantic Models**: Schemas de request/response 
3. **Basic Routing**: Endpoints básicos sem lógica

### Core API (Prioridade 2)  
4. **Async LangGraph Integration**: Adaptar workflow para FastAPI
5. **Comparison Endpoint**: Implementar POST /api/v1/compare
6. **Input Validation**: Validação robusta de payloads
7. **Error Handling**: Tratamento padronizado de erros

### Documentation & Discovery (Prioridade 3)
8. **Models Endpoint**: GET /api/v1/models
9. **Health Endpoint**: GET /api/v1/health  
10. **OpenAPI Enhancement**: Documentação rica com exemplos
11. **Swagger UI**: Interface interativa para testes

### Quality & Performance (Prioridade 4)
12. **API Tests**: Testes automatizados completos
13. **Performance Optimization**: Otimizações assíncronas
14. **Logging & Monitoring**: Observabilidade da API
15. **Deployment Setup**: Containerização e deploy

## Risks and Mitigations

### Technical Challenges
- **Risco**: Latência alta em requisições síncronas
- **Mitigação**: Implementação assíncrona com FastAPI e async/await

- **Risco**: Timeout em modelos LLM lentos  
- **Mitigação**: Timeouts configuráveis e cancelamento de requests

### API Design  
- **Risco**: Schema changes quebrando clientes
- **Mitigação**: Versionamento de API e backward compatibility

- **Risco**: Validação inadequada de inputs
- **Mitigação**: Pydantic com validação rigorosa e error messages claros

### Performance & Scalability
- **Risco**: Concorrência limitada
- **Mitigação**: FastAPI assíncrono com pool de connections

- **Risco**: Memory leaks em requests longas
- **Mitigação**: Proper cleanup e resource management

## Appendix

### Tech Stack
- **FastAPI**: Framework web assíncrono
- **Pydantic**: Validação e serialização
- **Uvicorn**: ASGI server
- **LangGraph**: Workflow orchestration  
- **OpenRouter**: LLM API access

### API Documentation Example
```yaml
openapi: 3.0.0
info:
  title: LLM as Judge API
  version: 1.0.0
  description: API para comparação de respostas entre modelos LLM
paths:
  /api/v1/compare:
    post:
      summary: Compara respostas de dois LLMs
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ComparisonRequest'
      responses:
        200:
          description: Comparação realizada com sucesso
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ComparisonResponse'
```

### Deployment Configuration
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY . .
RUN pip install uv && uv sync
EXPOSE 8000
CMD ["uv", "run", "uvicorn", "src.laaj.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Environment Variables
```bash
OPENROUTER_API_KEY=your_key_here
LANGSMITH_API_KEY=your_key_here  # Opcional
LANGSMITH_PROJECT_NAME=llm-as-judge-api
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=info
```