{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize FastAPI Project Structure",
        "description": "Finalize the existing FastAPI project structure to support the LLM as Judge Study system, integrating LangGraph with the FastAPI API.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "The project is a LLM as Judge Study system for comparing and evaluating responses from different language models using LangGraph. The base structure with src/laaj/ is already implemented with modules for agents, api, config, langsmith_integration, and workflow. Python 3.13+ is configured in pyproject.toml, dependencies are managed with uv, and environment variables are loaded with python-dotenv. Focus on completing the integration between the existing LangGraph system and the FastAPI API.",
        "testStrategy": "Run 'uvicorn src.laaj.api.main:app' and verify the server starts and responds to a root endpoint (e.g., /docs). Check environment variable loading and confirm the integration between FastAPI and LangGraph components.",
        "subtasks": [
          {
            "id": 2,
            "title": "Verify Python 3.13+ Local Environment",
            "description": "Verify that the local Python environment is using Python 3.13+ as specified in pyproject.toml.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Check the local Python version to ensure it meets the 3.13+ requirement specified in pyproject.toml. Update the local environment if necessary to match the project requirements.",
            "testStrategy": "Run 'python --version' and confirm it returns Python 3.13 or higher. Verify that the project runs correctly with this version."
          },
          {
            "id": 3,
            "title": "Configure Dependency Management with uv and Install Core Packages",
            "description": "Set up 'uv' as the dependency manager and install FastAPI (>=0.110), Uvicorn (>=0.29), and Pydantic v2 within the environment.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Add instructions to the Dockerfile to install 'uv' and use it to manage dependencies. Ensure FastAPI, Uvicorn, and Pydantic v2 are installed and listed in the dependency file.",
            "testStrategy": "Run 'uv pip list' or equivalent to confirm all required packages are installed at the correct versions."
          },
          {
            "id": 4,
            "title": "Integrate .env Support with python-dotenv",
            "description": "Install python-dotenv and configure the project to load environment variables from a .env file at startup.",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "Add python-dotenv to the dependencies and ensure the FastAPI app loads environment variables from .env using dotenv.load_dotenv or equivalent in the main entrypoint.",
            "testStrategy": "Set a test variable in .env, start the app, and verify the variable is accessible within the application."
          },
          {
            "id": 5,
            "title": "Set Up FastAPI Application with Uvicorn and Modular Router Support",
            "description": "Create the FastAPI app instance in src/main.py, configure Uvicorn as the ASGI server, and ensure the structure supports modular routers.",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "Implement src/main.py to instantiate FastAPI, include at least one example router, and configure the app to be served by Uvicorn. Ensure the structure allows for scalable router addition.",
            "testStrategy": "Run 'uvicorn src.main:app' and verify the server starts, responds to /docs, and loads environment variables from .env."
          },
          {
            "id": 6,
            "title": "Integrate LangGraph Workflow with FastAPI",
            "description": "Connect the existing LangGraph workflow components with the FastAPI application to enable LLM comparison functionality.",
            "status": "done",
            "dependencies": [
              5
            ],
            "details": "Create integration points between the FastAPI API and the LangGraph workflow components. Ensure the API can properly invoke the LangGraph agents and process their responses. Set up appropriate error handling and response formatting.",
            "testStrategy": "Test the integration by making API calls that trigger the LangGraph workflow and verify that responses are correctly processed and returned."
          },
          {
            "id": 7,
            "title": "Document Project Structure and API Integration",
            "description": "Create documentation explaining the project structure, the integration between FastAPI and LangGraph, and how to extend the system.",
            "status": "done",
            "dependencies": [
              6
            ],
            "details": "Document the src/laaj/ structure with its modules (agents, api, config, langsmith_integration, workflow). Explain how the FastAPI API interfaces with the LangGraph components. Include information on environment variables, dependency management, and how to add new features.",
            "testStrategy": "Review the documentation for completeness and accuracy. Ensure it provides clear guidance on the project structure and integration points."
          },
          {
            "id": 1,
            "title": "Create Base Project Directory and src/ Structure",
            "description": "Establish the root project directory and create a src/ folder for application code, ensuring a scalable and modular layout.",
            "dependencies": [],
            "details": "Set up the following structure: root directory with src/ for code, and subfolders for routers, schemas, and core modules. Include placeholder files such as __init__.py and main.py to support modular routers and future growth.",
            "status": "done",
            "testStrategy": "Verify that the directory structure matches best practices and that all required folders and files exist."
          }
        ]
      },
      {
        "id": 2,
        "title": "Define Pydantic Schemas for API Contracts",
        "description": "Implement all request and response models using Pydantic v2, strictly following the PRD's schema definitions for OpenAPI.",
        "details": "Create ComparisonRequest, ComparisonResponse, ModelInfo, and HealthResponse models in a schemas.py module. Use Pydantic v2's type annotations and Field metadata for OpenAPI documentation. Ensure all optional and required fields match the PRD. Add example payloads for OpenAPI docs. Validate datetime serialization using Python's standard datetime module.",
        "testStrategy": "Write unit tests to validate schema parsing and serialization. Use FastAPI's OpenAPI docs to verify schema rendering and field descriptions.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract and Analyze PRD Schema Definitions",
            "description": "Review the Product Requirements Document (PRD) to identify and document all required fields, types, and constraints for ComparisonRequest, ComparisonResponse, ModelInfo, and HealthResponse models.",
            "dependencies": [],
            "details": "Ensure all optional and required fields, data types, and constraints are clearly mapped for each model as per the PRD. Note any special requirements for OpenAPI compatibility or field documentation.",
            "status": "done",
            "testStrategy": "Cross-check extracted schema definitions with the PRD and confirm completeness with stakeholders if needed."
          },
          {
            "id": 2,
            "title": "Implement Pydantic v2 Models with Type Annotations",
            "description": "Define the ComparisonRequest, ComparisonResponse, ModelInfo, and HealthResponse classes in schemas.py using Pydantic v2, applying precise type annotations and Field metadata.",
            "dependencies": [
              "2.1"
            ],
            "details": "Use Pydantic v2 syntax, including Annotated types and Field for metadata. Ensure all fields match the PRD in type, required/optional status, and constraints. Use Python's standard datetime module for datetime fields.",
            "status": "done",
            "testStrategy": "Write unit tests to instantiate each model with valid and invalid data, verifying type enforcement and required/optional field handling."
          },
          {
            "id": 3,
            "title": "Add OpenAPI Documentation Metadata and Example Payloads",
            "description": "Enhance each Pydantic model with descriptive metadata and example payloads for OpenAPI documentation using Field and model_config.",
            "dependencies": [
              "2.2"
            ],
            "details": "Provide field descriptions, example values, and model-level examples using Field and model_config's json_schema_extra. Ensure OpenAPI docs generated by FastAPI reflect these enhancements.",
            "status": "done",
            "testStrategy": "Inspect the generated OpenAPI schema via FastAPI's /docs endpoint and verify that all descriptions and examples appear as intended."
          },
          {
            "id": 4,
            "title": "Validate Datetime Serialization and Custom Types",
            "description": "Ensure all datetime fields serialize and deserialize correctly using Python's standard datetime module, and implement any necessary custom types or validators for OpenAPI compatibility.",
            "dependencies": [
              "2.2"
            ],
            "details": "Test serialization and parsing of datetime fields in request and response models. If custom types are needed, use Pydantic v2's Annotated and custom schema hooks.",
            "status": "done",
            "testStrategy": "Write tests for datetime field round-trip serialization and deserialization. Confirm OpenAPI schema correctly documents datetime formats."
          },
          {
            "id": 5,
            "title": "Verify Schema Compliance and Integration with FastAPI",
            "description": "Integrate the schemas.py module with FastAPI, ensuring all endpoints use the correct models and that the OpenAPI schema matches the PRD definitions.",
            "dependencies": [
              "2.3",
              "2.4"
            ],
            "details": "Update FastAPI endpoint signatures to use the new models. Use FastAPI's OpenAPI docs to verify schema rendering, field descriptions, and example payloads. Address any discrepancies with the PRD.",
            "status": "done",
            "testStrategy": "Perform end-to-end tests by sending requests to each endpoint and validating both the OpenAPI schema and actual request/response payloads."
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Core Routing and Middleware",
        "description": "Set up API routers for /api/v1/compare, /api/v1/models, /api/v1/health, and /docs. Add CORS middleware and basic request logging.",
        "details": "Use FastAPI's APIRouter for modular endpoint organization. Configure CORS with fastapi.middleware.cors for broad compatibility. Add logging middleware using Python's logging module with structured JSON output. Ensure all endpoints are versioned under /api/v1. Prepare for future authentication middleware but do not implement it yet.",
        "testStrategy": "Send requests to each endpoint and verify correct routing, CORS headers, and log output. Confirm /docs is accessible and interactive.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Modular APIRouters for Endpoints",
            "description": "Define separate APIRouter instances for /api/v1/compare, /api/v1/models, /api/v1/health, and /docs, organizing each in its own module for maintainability.",
            "dependencies": [],
            "details": "Use FastAPI's APIRouter to modularize endpoint definitions, placing each router in a dedicated Python file and including them in the main application under the /api/v1 prefix.",
            "status": "done",
            "testStrategy": "Send requests to each endpoint and verify correct routing and response structure."
          },
          {
            "id": 2,
            "title": "Integrate APIRouters into Main FastAPI Application",
            "description": "Mount all defined APIRouters to the main FastAPI app, ensuring each endpoint is versioned under /api/v1 and /docs is accessible.",
            "dependencies": [
              "3.1"
            ],
            "details": "Use include_router for each APIRouter, setting the appropriate prefix and tags. Confirm that all endpoints are grouped and discoverable via OpenAPI docs.",
            "status": "done",
            "testStrategy": "Access /docs and all /api/v1 endpoints to confirm correct integration and documentation."
          },
          {
            "id": 3,
            "title": "Configure CORS Middleware for Broad Compatibility",
            "description": "Add and configure FastAPI's CORS middleware to allow requests from any origin, supporting all standard HTTP methods and headers.",
            "dependencies": [
              "3.2"
            ],
            "details": "Use fastapi.middleware.cors.CORSMiddleware with permissive settings to maximize compatibility for frontend and external clients.",
            "status": "done",
            "testStrategy": "Send cross-origin requests from various origins and verify CORS headers in responses."
          },
          {
            "id": 4,
            "title": "Implement Structured JSON Logging Middleware",
            "description": "Add middleware that logs each incoming request and response using Python's logging module, outputting logs in structured JSON format.",
            "dependencies": [
              "3.3"
            ],
            "details": "Create a custom FastAPI middleware that captures request details, response status, and timestamps, formatting logs as JSON for downstream analysis.",
            "status": "done",
            "testStrategy": "Trigger requests to all endpoints and verify that logs are correctly structured and contain required fields."
          },
          {
            "id": 5,
            "title": "Prepare for Future Authentication Middleware Integration",
            "description": "Design the middleware stack to allow seamless addition of authentication middleware in the future, without implementing authentication logic yet.",
            "dependencies": [
              "3.4"
            ],
            "details": "Ensure middleware ordering and router structure are compatible with future authentication requirements, documenting integration points for later development.",
            "status": "done",
            "testStrategy": "Review middleware and router setup to confirm that authentication can be added without refactoring."
          }
        ]
      },
      {
        "id": 4,
        "title": "Integrate Async LangGraph Workflow Engine",
        "description": "Integrate LangGraph for orchestrating LLM and judge model calls asynchronously, supporting both real-time and pre-generated response flows.",
        "details": "Install langgraph (latest stable). Implement async workflows for LLM comparison: if responses are not provided, call LLMs via OpenRouter; otherwise, use provided responses. Ensure judge model is invoked asynchronously. Implement timeout and cancellation logic using asyncio and FastAPI's background tasks. Use OpenRouter API for LLM access, with API keys from environment variables.",
        "testStrategy": "Mock LLM and judge calls for unit tests. Test concurrent requests and timeout handling. Validate correct async execution and error propagation.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Install and Configure LangGraph and Dependencies",
            "description": "Install the latest stable version of LangGraph and all required dependencies, including langchain-openai and python-dotenv. Set up environment variable management for OpenRouter API keys.",
            "dependencies": [],
            "details": "Ensure all packages are installed in the project environment. Verify that environment variables for API keys are loaded securely using python-dotenv.",
            "status": "pending",
            "testStrategy": "Run a minimal LangGraph workflow to confirm installation and environment variable access."
          },
          {
            "id": 2,
            "title": "Design Async LangGraph Workflow for LLM and Judge Orchestration",
            "description": "Define the LangGraph workflow structure to orchestrate asynchronous calls to LLMs and the judge model, supporting both real-time and pre-generated response flows.",
            "dependencies": [
              "4.1"
            ],
            "details": "Model the workflow graph to branch based on whether LLM responses are provided or need to be generated. Ensure nodes for LLM and judge calls are designed for async execution.",
            "status": "pending",
            "testStrategy": "Create unit tests for workflow branching logic and validate correct node execution paths."
          },
          {
            "id": 3,
            "title": "Implement Async LLM and Judge Model Invocation",
            "description": "Develop async functions to invoke LLMs via OpenRouter and the judge model, integrating them as nodes in the LangGraph workflow.",
            "dependencies": [
              "4.2"
            ],
            "details": "Use asyncio for concurrency and ensure that both LLM and judge calls are non-blocking. Handle both real-time generation and usage of pre-generated responses.",
            "status": "pending",
            "testStrategy": "Mock LLM and judge endpoints to test async invocation and response handling."
          },
          {
            "id": 4,
            "title": "Integrate Timeout and Cancellation Logic",
            "description": "Add timeout and cancellation support to the async workflow using asyncio and FastAPI background tasks, ensuring robust handling of long-running or stalled operations.",
            "dependencies": [
              "4.3"
            ],
            "details": "Implement timeout wrappers for async tasks and cancellation hooks for FastAPI background tasks. Ensure that all workflow branches respect timeout and cancellation signals.",
            "status": "pending",
            "testStrategy": "Simulate slow or unresponsive LLM/judge calls and verify that timeouts and cancellations are triggered and handled gracefully."
          },
          {
            "id": 5,
            "title": "Validate Workflow Integration and Error Propagation",
            "description": "Test the complete async LangGraph workflow within the FastAPI context, ensuring correct orchestration, error propagation, and compatibility with downstream endpoints.",
            "dependencies": [
              "4.4"
            ],
            "details": "Integrate the workflow into the FastAPI app, validate correct execution for both real-time and pre-generated flows, and ensure errors are propagated and logged according to project standards.",
            "status": "pending",
            "testStrategy": "Run integration tests with concurrent requests, covering success, timeout, cancellation, and error scenarios. Verify correct responses and error handling."
          }
        ]
      },
      {
        "id": 5,
        "title": "Develop POST /api/v1/compare Endpoint",
        "description": "Implement the main comparison endpoint, supporting both real-time generation and pre-generated responses, with dynamic model selection and robust validation.",
        "details": "Accept ComparisonRequest payloads. Validate that at least one of (llm_a, llm_a_response) and (llm_b, llm_b_response) is provided. Dynamically instantiate LLM and judge models via OpenRouter. Use LangGraph workflow for orchestration. Return ComparisonResponse with all required fields, including execution_time and timestamp. Handle errors with standardized responses.",
        "testStrategy": "Write integration tests for both real-time and pre-generated flows. Validate response structure, error handling, and edge cases (e.g., missing fields, invalid models).",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Document ComparisonRequest and ComparisonResponse Schemas",
            "description": "Define the request and response payload schemas for the /api/v1/compare endpoint, ensuring all required fields (including execution_time and timestamp) are present and clearly documented.",
            "dependencies": [],
            "details": "Specify field types, validation rules, and example payloads for both real-time and pre-generated comparison flows. Ensure OpenAPI documentation is updated for discoverability and clarity.",
            "status": "pending",
            "testStrategy": "Validate schema correctness using schema validation tools and ensure OpenAPI docs render example requests and responses accurately."
          },
          {
            "id": 2,
            "title": "Implement Robust Input Validation Logic",
            "description": "Develop validation logic to ensure that at least one of (llm_a, llm_a_response) and (llm_b, llm_b_response) is provided, and all required fields are present and correctly formatted.",
            "dependencies": [
              "5.1"
            ],
            "details": "Use FastAPI's validation features and custom logic to enforce input requirements. Return standardized error responses for missing or invalid fields.",
            "status": "pending",
            "testStrategy": "Write unit and integration tests for edge cases such as missing fields, invalid types, and malformed payloads. Confirm error responses match specification."
          },
          {
            "id": 3,
            "title": "Integrate Dynamic Model Instantiation via OpenRouter",
            "description": "Implement logic to dynamically instantiate LLM and judge models using OpenRouter based on the incoming request parameters.",
            "dependencies": [
              "5.2"
            ],
            "details": "Support selection of models for both real-time generation and pre-generated responses. Handle model availability and selection errors gracefully.",
            "status": "pending",
            "testStrategy": "Test model instantiation with various valid and invalid model identifiers. Mock OpenRouter responses to verify error handling and fallback behavior."
          },
          {
            "id": 4,
            "title": "Orchestrate Comparison Workflow Using LangGraph",
            "description": "Integrate LangGraph workflow to manage orchestration of comparison tasks, including invoking LLMs, collecting responses, and applying judge models.",
            "dependencies": [
              "5.3"
            ],
            "details": "Ensure seamless workflow execution for both real-time and pre-generated flows. Capture execution_time and timestamp for each comparison.",
            "status": "pending",
            "testStrategy": "Write integration tests for workflow execution paths, verifying correct orchestration, timing, and response aggregation."
          },
          {
            "id": 5,
            "title": "Implement Endpoint Logic and Standardized Error Handling",
            "description": "Develop the POST /api/v1/compare endpoint, wiring together validation, model instantiation, workflow orchestration, and standardized error handling.",
            "dependencies": [
              "5.4"
            ],
            "details": "Ensure endpoint returns a fully populated ComparisonResponse, including execution_time and timestamp. Handle all errors with consistent, documented response formats.",
            "status": "pending",
            "testStrategy": "Write end-to-end tests for both success and failure scenarios, including malformed requests, invalid models, and workflow errors. Validate response structure and error consistency."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Model Registry and Validation Logic",
        "description": "Create a registry of available LLM and judge models, supporting dynamic discovery and validation for incoming requests.",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "details": "Maintain a Python dictionary or config file listing supported models, their providers, and metadata. Expose this registry to the /api/v1/models endpoint. Validate incoming model names in /compare requests against the registry. Support hot-reload or config-driven updates for future extensibility. Integrate with the existing FastAPI structure in src/laaj/api/.",
        "testStrategy": "Test model listing, validation errors for unknown models, and correct metadata in responses. Add unit tests for registry logic.",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Model Registry Data Structure",
            "description": "Define the schema and structure for storing model information, including model names, providers, types (LLM/judge), and metadata. Choose between a Python dictionary or a config file for initial implementation.",
            "status": "pending",
            "dependencies": [],
            "details": "Specify required metadata fields such as version, description, owner, and performance metrics. Ensure extensibility for future model attributes. Create Pydantic models in src/laaj/api/schemas/ for model registry data representation.",
            "testStrategy": "Validate schema correctness and extensibility by adding sample models and verifying metadata integrity."
          },
          {
            "id": 2,
            "title": "Implement Registry Loading and Hot-Reload Logic",
            "description": "Develop logic to load the model registry from the chosen data source at application startup and support hot-reload or config-driven updates without downtime.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Monitor the config file or registry source for changes and reload the registry in-memory when updates are detected. Ensure thread safety and minimal performance impact. Implement this logic within the appropriate module in the src/laaj/api/ structure.",
            "testStrategy": "Test registry reload by updating the config and verifying that changes are reflected in API responses without restarting the service."
          },
          {
            "id": 3,
            "title": "Create Models Router for /api/v1/models Endpoint",
            "description": "Create a dedicated router in src/laaj/api/routers/ that exposes the current registry of available models, including all relevant metadata.",
            "status": "pending",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement the router using FastAPI's APIRouter, ensuring the response matches the defined schema and includes all model metadata. Register this router in main.py to maintain the project's architecture.",
            "testStrategy": "Send requests to /api/v1/models and verify correct listing, metadata accuracy, and schema compliance."
          },
          {
            "id": 4,
            "title": "Integrate Model Validation with Comparison Router",
            "description": "Implement logic to validate incoming model names in /compare requests against the registry, returning errors for unknown or unsupported models.",
            "status": "pending",
            "dependencies": [
              1,
              2
            ],
            "details": "Integrate validation into the comparison router in src/laaj/api/routers/. Ensure the validation works with the LangGraph workflow. Return descriptive error messages for invalid model names and ensure only registered models are accepted.",
            "testStrategy": "Test /compare requests with valid and invalid model names, verifying correct validation and error handling."
          },
          {
            "id": 5,
            "title": "Develop Unit Tests for Registry and Validation Logic",
            "description": "Write comprehensive unit tests for registry loading, hot-reload, model listing, and validation logic to ensure reliability and correctness.",
            "status": "pending",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Use pytest to cover all edge cases, including registry updates, invalid requests, and metadata accuracy. Achieve high test coverage for all registry-related code. Follow the project's testing structure and conventions.",
            "testStrategy": "Run tests locally and in CI, ensuring all tests pass and failures are actionable. Monitor coverage metrics for registry logic."
          },
          {
            "id": 6,
            "title": "Update app.py as Entry Point",
            "description": "Configure app.py as the main entry point that imports and uses the FastAPI application defined in src/laaj/api/main.py.",
            "status": "pending",
            "dependencies": [
              3,
              4
            ],
            "details": "Ensure app.py properly imports the FastAPI application from main.py and handles any necessary initialization for the model registry. This maintains the separation of concerns in the project architecture.",
            "testStrategy": "Verify that the application starts correctly with app.py as the entry point and that all endpoints are accessible."
          }
        ]
      },
      {
        "id": 7,
        "title": "Standardize Error Handling and Response Formatting",
        "description": "Implement global exception handlers for validation, timeout, and internal errors, ensuring all responses follow a consistent JSON structure.",
        "details": "Use FastAPI's exception handling to catch RequestValidationError, HTTPException, and generic Exception. Return error responses with status, message, and details fields. Log all errors with context. Document error schemas in OpenAPI. Ensure error codes are mapped to actionable messages.",
        "testStrategy": "Trigger various error scenarios (invalid input, timeouts, internal errors) and verify standardized error responses and logs.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Standardized Error Response Schema",
            "description": "Define a consistent JSON structure for all error responses, including required fields such as status, message, and details.",
            "dependencies": [],
            "details": "Specify the schema for error responses to be used across all exception handlers. Ensure the schema is expressive enough to cover validation, timeout, and internal errors.",
            "status": "pending",
            "testStrategy": "Review schema with stakeholders and validate against example error scenarios."
          },
          {
            "id": 2,
            "title": "Implement Global Exception Handlers",
            "description": "Create global exception handlers in FastAPI for RequestValidationError, HTTPException, and generic Exception, returning responses in the standardized format.",
            "dependencies": [
              "7.1"
            ],
            "details": "Use FastAPI's @app.exception_handler decorator to register handlers for each exception type. Ensure each handler returns the standardized JSON structure and appropriate HTTP status codes.",
            "status": "pending",
            "testStrategy": "Trigger each exception type and verify the response matches the schema and contains correct status codes."
          },
          {
            "id": 3,
            "title": "Integrate Contextual Error Logging",
            "description": "Log all errors with relevant context, including request path, parameters, and stack traces, without exposing sensitive information.",
            "dependencies": [
              "7.2"
            ],
            "details": "Enhance each exception handler to log error details using a centralized logging utility. Ensure logs include contextual information for debugging and monitoring.",
            "status": "pending",
            "testStrategy": "Simulate errors and verify logs contain necessary context while omitting sensitive data."
          },
          {
            "id": 4,
            "title": "Document Error Schemas in OpenAPI",
            "description": "Add error response schemas and examples to the OpenAPI documentation for all endpoints.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "Use FastAPI's response_model and OpenAPI customization features to document error responses. Provide schema definitions and example error payloads for each error type.",
            "status": "pending",
            "testStrategy": "Inspect generated OpenAPI docs (Swagger UI) to confirm error schemas and examples are present and accurate."
          },
          {
            "id": 5,
            "title": "Map Error Codes to Actionable Messages",
            "description": "Establish a mapping between error codes and user-friendly, actionable error messages for all handled exceptions.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "Create a mapping layer or utility that translates internal error codes to clear, actionable messages included in the error response payload.",
            "status": "pending",
            "testStrategy": "Trigger errors with various codes and verify that responses contain the correct, actionable messages."
          }
        ]
      },
      {
        "id": 8,
        "title": "Build GET /api/v1/models Endpoint",
        "description": "Expose a GET endpoint that returns the list of available LLM and judge models with metadata for discovery.",
        "details": "Return a list of ModelInfo objects from the model registry. Support filtering or future expansion (e.g., by provider or availability). Ensure OpenAPI docs include example responses.",
        "testStrategy": "Test endpoint response for accuracy, completeness, and correct OpenAPI documentation.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define ModelInfo Schema and Metadata",
            "description": "Design the ModelInfo object structure to include all relevant metadata for LLM and judge models, ensuring extensibility for future filtering and provider-specific fields.",
            "dependencies": [],
            "details": "Specify required and optional fields such as model name, provider, type, availability, and any additional metadata needed for discovery. Document the schema for use in endpoint responses and OpenAPI examples.",
            "status": "pending",
            "testStrategy": "Validate schema correctness and completeness against registry data; ensure all required fields are present in example responses."
          },
          {
            "id": 2,
            "title": "Implement GET /api/v1/models Endpoint Logic",
            "description": "Develop the FastAPI route handler for GET /api/v1/models to retrieve and return the list of ModelInfo objects from the model registry.",
            "dependencies": [
              "8.1"
            ],
            "details": "Query the model registry for available models, serialize ModelInfo objects, and return them in a standardized JSON response. Ensure proper HTTP status codes and error handling for edge cases.",
            "status": "pending",
            "testStrategy": "Test endpoint for correct response structure, status codes, and error handling when registry is empty or unavailable."
          },
          {
            "id": 3,
            "title": "Add Filtering and Query Parameter Support",
            "description": "Enable optional filtering of models by provider, type, or availability using query parameters, preparing for future expansion.",
            "dependencies": [
              "8.2"
            ],
            "details": "Parse query parameters, apply filters to the model registry, and return only matching ModelInfo objects. Document supported filters in OpenAPI docs.",
            "status": "pending",
            "testStrategy": "Test endpoint with various filter combinations; verify correct models are returned and invalid filters are handled gracefully."
          },
          {
            "id": 4,
            "title": "Integrate with Model Registry for Dynamic Discovery",
            "description": "Ensure the endpoint dynamically reflects the current state of the model registry, supporting hot-reload or config-driven updates.",
            "dependencies": [
              "8.3"
            ],
            "details": "Connect the endpoint to the registry implementation, ensuring updates to the registry are immediately reflected in API responses without requiring a restart.",
            "status": "pending",
            "testStrategy": "Update registry data and verify endpoint response updates in real time; test for consistency and accuracy."
          },
          {
            "id": 5,
            "title": "Document Endpoint in OpenAPI with Example Responses",
            "description": "Add comprehensive OpenAPI documentation for the GET /api/v1/models endpoint, including schema definitions, query parameters, and example responses.",
            "dependencies": [
              "8.4"
            ],
            "details": "Ensure OpenAPI docs include detailed descriptions, request/response schemas, filtering options, and realistic example payloads for discovery.",
            "status": "pending",
            "testStrategy": "Verify OpenAPI docs render correctly, include all required details, and example responses match actual endpoint output."
          }
        ]
      },
      {
        "id": 9,
        "title": "Build GET /api/v1/health Endpoint",
        "description": "Implement a health check endpoint that reports API status, version, available models count, and uptime.",
        "details": "Track application start time for uptime calculation. Return HealthResponse with status, version (from config), available_models (from registry), and uptime (in seconds). Ensure endpoint is lightweight and fast.",
        "testStrategy": "Test endpoint under normal and degraded conditions (e.g., model registry unavailable). Validate uptime and version accuracy.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define HealthResponse Schema and Endpoint Contract",
            "description": "Design the HealthResponse data model to include status, version, available_models, and uptime fields. Specify the OpenAPI contract for GET /api/v1/health, ensuring clarity on response structure and field types.",
            "dependencies": [],
            "details": "Ensure the schema is lightweight and includes all required fields. Document expected response codes and example payloads for OpenAPI docs.",
            "status": "pending",
            "testStrategy": "Validate schema correctness and OpenAPI documentation using automated tools. Confirm all required fields are present and correctly typed."
          },
          {
            "id": 2,
            "title": "Implement Application Uptime Tracking",
            "description": "Add logic to record application start time and calculate uptime in seconds for each health check request.",
            "dependencies": [
              "9.1"
            ],
            "details": "Store the start time at application initialization. On each GET /api/v1/health call, compute uptime as the difference between current time and start time.",
            "status": "pending",
            "testStrategy": "Test uptime calculation accuracy by comparing reported uptime to actual elapsed time after various delays."
          },
          {
            "id": 3,
            "title": "Integrate Version and Model Registry Data",
            "description": "Fetch API version from configuration and available_models count from the model registry for inclusion in the health response.",
            "dependencies": [
              "9.1"
            ],
            "details": "Ensure version is dynamically loaded from config and available_models reflects the current registry state. Handle registry unavailability gracefully.",
            "status": "pending",
            "testStrategy": "Test with different config versions and simulate model registry changes and failures to verify correct reporting."
          },
          {
            "id": 4,
            "title": "Implement GET /api/v1/health Endpoint Logic",
            "description": "Develop the FastAPI route handler for GET /api/v1/health, assembling the HealthResponse from status, version, available_models, and uptime.",
            "dependencies": [
              "9.2",
              "9.3"
            ],
            "details": "Ensure the endpoint is lightweight, fast, and does not perform heavy computations or blocking I/O. Return HTTP 200 with the HealthResponse payload.",
            "status": "pending",
            "testStrategy": "Benchmark endpoint response time under normal and high-load conditions. Confirm correct response structure and status codes."
          },
          {
            "id": 5,
            "title": "Test Endpoint Under Normal and Degraded Conditions",
            "description": "Develop and execute tests for the health endpoint, covering normal operation and scenarios where dependencies (e.g., model registry) are unavailable.",
            "dependencies": [
              "9.4"
            ],
            "details": "Automate tests to validate uptime, version accuracy, available_models count, and graceful degradation. Ensure endpoint remains responsive and informative during failures.",
            "status": "pending",
            "testStrategy": "Use automated test suites to simulate dependency failures, verify error handling, and confirm endpoint reliability and correctness."
          }
        ]
      },
      {
        "id": 10,
        "title": "Enhance OpenAPI Documentation and Swagger UI",
        "description": "Provide rich, interactive API documentation with examples, detailed field descriptions, and error schemas using FastAPI's OpenAPI integration.",
        "details": "Annotate all endpoints and models with descriptions and examples. Use FastAPI's openapi_extra and response_model_exclude_unset for clarity. Ensure /docs is enabled and styled. Add example requests and responses for all endpoints. Document error responses and edge cases.",
        "testStrategy": "Review /docs for completeness, clarity, and usability. Solicit feedback from target personas (developer, researcher, analyst).",
        "priority": "medium",
        "dependencies": [
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Annotate Endpoints and Models with Descriptions and Examples",
            "description": "Add detailed descriptions and example values to all FastAPI endpoints and Pydantic models to enhance the generated OpenAPI schema.",
            "dependencies": [],
            "details": "Use Pydantic's Field metadata and FastAPI's docstrings to provide clear, concise descriptions and realistic example values for every request and response field. Ensure all endpoints and models are annotated for maximum clarity in the docs.",
            "status": "pending",
            "testStrategy": "Review the OpenAPI schema and Swagger UI to confirm that all fields and endpoints display appropriate descriptions and examples."
          },
          {
            "id": 2,
            "title": "Implement Example Requests and Responses for All Endpoints",
            "description": "Provide example request and response payloads for every API endpoint to illustrate typical usage and expected data formats.",
            "dependencies": [
              "10.1"
            ],
            "details": "Leverage FastAPI's openapi_extra and response_model_exclude_unset to add example payloads directly in endpoint definitions. Ensure that both successful and error scenarios are represented.",
            "status": "pending",
            "testStrategy": "Verify that Swagger UI displays example requests and responses for each endpoint, and that these examples are accurate and helpful."
          },
          {
            "id": 3,
            "title": "Document Error Responses and Edge Cases",
            "description": "Define and document all possible error responses, including status codes, error schemas, and edge case scenarios for each endpoint.",
            "dependencies": [
              "10.1"
            ],
            "details": "Use FastAPI's response_model and responses parameters to specify error response models and descriptions. Clearly document edge cases and validation errors, ensuring they are visible in the OpenAPI docs.",
            "status": "pending",
            "testStrategy": "Check that all documented error responses appear in Swagger UI, and that their schemas and descriptions match actual API behavior."
          },
          {
            "id": 4,
            "title": "Configure and Style Swagger UI Documentation",
            "description": "Ensure the /docs endpoint is enabled, accessible, and visually styled for usability, including custom branding or theming if required.",
            "dependencies": [
              "10.1"
            ],
            "details": "Configure FastAPI's docs_url and related parameters to serve Swagger UI at the desired path. Apply custom styling or branding as needed to improve developer experience.",
            "status": "pending",
            "testStrategy": "Access /docs and confirm it is enabled, styled as intended, and fully functional across browsers."
          },
          {
            "id": 5,
            "title": "Review and Validate Documentation Completeness and Usability",
            "description": "Conduct a thorough review of the generated OpenAPI documentation for completeness, clarity, and usability, incorporating feedback from target personas.",
            "dependencies": [
              "10.2",
              "10.3",
              "10.4"
            ],
            "details": "Solicit feedback from developers, researchers, and analysts. Iterate on documentation based on their input to ensure all information is discoverable and actionable.",
            "status": "pending",
            "testStrategy": "Perform walkthroughs with target users, collect feedback, and verify that all endpoints, models, examples, and error cases are clearly documented and easy to understand."
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Automated API Testing Suite",
        "description": "Develop a comprehensive suite of automated tests using pytest and pytest-httpx for endpoint, schema, and error validation.",
        "details": "Write tests for all endpoints, including success and failure cases. Use pytest-httpx to mock external LLM/judge calls. Achieve high coverage for input validation, error handling, and concurrency. Integrate tests into CI pipeline (e.g., GitHub Actions).",
        "testStrategy": "Run tests locally and in CI. Ensure all tests pass and failures are actionable. Monitor coverage metrics.",
        "priority": "high",
        "dependencies": [
          5,
          7,
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Test Coverage Plan for API Endpoints",
            "description": "Identify all API endpoints and specify required test cases for success, failure, input validation, error handling, and concurrency scenarios.",
            "dependencies": [],
            "details": "Review the API specification and enumerate endpoints. For each endpoint, outline test cases covering valid and invalid inputs, expected errors, and concurrent access requirements.",
            "status": "pending",
            "testStrategy": "Verify completeness of the test plan by cross-referencing with the API documentation and coverage metrics."
          },
          {
            "id": 2,
            "title": "Implement Endpoint and Schema Validation Tests Using Pytest",
            "description": "Write pytest test functions for each endpoint, validating correct responses, status codes, and schema adherence for both success and failure cases.",
            "dependencies": [
              "11.1"
            ],
            "details": "Use pytest to structure tests and assert response status codes and payloads. Validate response schemas against Pydantic models to ensure contract compliance.",
            "status": "pending",
            "testStrategy": "Run tests locally and confirm all endpoints return expected results and schemas for defined scenarios."
          },
          {
            "id": 3,
            "title": "Mock External LLM/Judge Calls with pytest-httpx",
            "description": "Integrate pytest-httpx to mock HTTPX requests for external LLM and judge model calls, simulating various response and error conditions.",
            "dependencies": [
              "11.2"
            ],
            "details": "Configure pytest-httpx fixtures to intercept and mock outbound HTTPX calls. Add responses and exceptions to test handling of timeouts, errors, and edge cases.",
            "status": "pending",
            "testStrategy": "Ensure all external calls are mocked and verify correct error handling and fallback logic in tests."
          },
          {
            "id": 4,
            "title": "Test Input Validation, Error Handling, and Concurrency",
            "description": "Develop targeted tests for input validation, error scenarios, and concurrent requests to achieve high coverage and robustness.",
            "dependencies": [
              "11.3"
            ],
            "details": "Write tests for invalid payloads, missing fields, and boundary conditions. Simulate concurrent requests to validate thread safety and race condition handling.",
            "status": "pending",
            "testStrategy": "Monitor coverage metrics and run stress/concurrency tests to confirm error handling and input validation under load."
          },
          {
            "id": 5,
            "title": "Integrate Test Suite into CI Pipeline",
            "description": "Configure the automated test suite to run in the CI pipeline (e.g., GitHub Actions), ensuring all tests execute on each commit and failures are actionable.",
            "dependencies": [
              "11.4"
            ],
            "details": "Set up CI configuration to install dependencies, run pytest, and report results. Monitor test outcomes and coverage metrics in CI dashboards.",
            "status": "pending",
            "testStrategy": "Validate that tests run automatically in CI, failures are reported clearly, and coverage thresholds are enforced."
          }
        ]
      },
      {
        "id": 12,
        "title": "Containerize and Prepare for Deployment",
        "description": "Create a production-ready Dockerfile and deployment configuration, following best practices for FastAPI and Uvicorn.",
        "details": "Use the provided Dockerfile as a base. Ensure multi-stage builds for smaller images. Set up healthcheck in Docker. Configure environment variables via .env and Docker secrets. Prepare for deployment on cloud platforms (e.g., AWS ECS, Azure Container Apps). Document deployment steps and environment requirements.",
        "testStrategy": "Build and run the Docker image locally. Deploy to a staging environment and verify all endpoints, health, and logging. Test environment variable injection and healthcheck.",
        "priority": "medium",
        "dependencies": [
          1,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor Dockerfile for Production Readiness",
            "description": "Update the provided Dockerfile to follow best practices for FastAPI and Uvicorn, including multi-stage builds, use of a slim Python base image, and Gunicorn as a process manager.",
            "dependencies": [],
            "details": "Ensure the Dockerfile uses multi-stage builds to minimize image size, installs dependencies efficiently, and sets up Gunicorn with UvicornWorker for robust production serving. Use the exec form for CMD and include a .dockerignore file.",
            "status": "pending",
            "testStrategy": "Build the Docker image locally and verify that the container starts and serves the FastAPI application using Gunicorn and UvicornWorker."
          },
          {
            "id": 2,
            "title": "Implement Docker Healthcheck and Environment Configuration",
            "description": "Add a HEALTHCHECK instruction to the Dockerfile and configure environment variables using .env files and Docker secrets.",
            "dependencies": [
              "12.1"
            ],
            "details": "Define a healthcheck command in the Dockerfile to monitor application health. Ensure environment variables are loaded via python-dotenv and support Docker secrets for sensitive data.",
            "status": "pending",
            "testStrategy": "Run the container and verify healthcheck status via 'docker inspect'. Test environment variable injection and secret loading in both local and staging environments."
          },
          {
            "id": 3,
            "title": "Prepare Deployment Configuration for Cloud Platforms",
            "description": "Create deployment configuration files for cloud platforms such as AWS ECS and Azure Container Apps, ensuring compatibility and scalability.",
            "dependencies": [
              "12.2"
            ],
            "details": "Write production-ready docker-compose and/or cloud-specific configuration files. Set up service definitions, environment variable mapping, and resource limits. Ensure readiness for scaling and secure deployment.",
            "status": "pending",
            "testStrategy": "Deploy the container to a staging environment on the target cloud platform and verify service startup, endpoint accessibility, and resource allocation."
          },
          {
            "id": 4,
            "title": "Document Deployment Steps and Environment Requirements",
            "description": "Write comprehensive documentation detailing the deployment process, environment setup, and configuration requirements for production.",
            "dependencies": [
              "12.3"
            ],
            "details": "Create a deployment guide covering Docker build steps, environment variable management, healthcheck usage, and cloud deployment instructions. Include prerequisites and troubleshooting tips.",
            "status": "pending",
            "testStrategy": "Review documentation for completeness and clarity. Have a team member follow the guide to deploy the application from scratch."
          },
          {
            "id": 5,
            "title": "Validate Production Container in Staging Environment",
            "description": "Test the production container in a staging environment to verify endpoint functionality, healthcheck, logging, and environment variable handling.",
            "dependencies": [
              "12.4"
            ],
            "details": "Deploy the container to a staging environment, run integration tests against all endpoints, and monitor healthcheck and logging outputs. Confirm correct environment variable injection and secure handling of secrets.",
            "status": "pending",
            "testStrategy": "Execute automated and manual tests to ensure all endpoints respond correctly, healthcheck passes, and logs are properly generated. Validate environment variable and secret management."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-10T21:50:09.672Z",
      "updated": "2025-08-10T23:15:15.421Z",
      "description": "Tasks for master context"
    }
  }
}