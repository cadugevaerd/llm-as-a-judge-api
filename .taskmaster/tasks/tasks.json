{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Create Batch Endpoint Structure",
        "description": "Implement the new endpoint for batch processing of comparisons in the LaaJ API",
        "details": "1. Add a new route handler for POST /api/v1/compare/batch\n2. Implement request validation for the batch structure\n3. Set up the controller function to handle batch requests\n4. Implement input validation to ensure maximum 5 comparisons per batch\n5. Ensure the endpoint follows the same authentication/authorization pattern as existing endpoints\n6. Return appropriate error responses for invalid requests\n\nCode structure:\n```javascript\n// routes/compare.js\nrouter.post('/batch', validateBatchRequest, compareBatchController);\n\n// middleware/validators.js\nfunction validateBatchRequest(req, res, next) {\n  const { comparisons } = req.body;\n  \n  if (!Array.isArray(comparisons)) {\n    return res.status(400).json({ error: 'Comparisons must be an array' });\n  }\n  \n  if (comparisons.length > 5) {\n    return res.status(400).json({ error: 'Maximum 5 comparisons per batch' });\n  }\n  \n  if (comparisons.length === 0) {\n    return res.status(400).json({ error: 'At least one comparison is required' });\n  }\n  \n  // Validate each comparison object\n  for (const comp of comparisons) {\n    if (!comp.input || !comp.response_a || !comp.response_b) {\n      return res.status(400).json({ error: 'Each comparison must include input, response_a, and response_b' });\n    }\n  }\n  \n  next();\n}\n```",
        "testStrategy": "1. Unit tests for the validation middleware to ensure it correctly validates batch requests\n2. Integration tests for the endpoint to verify it accepts valid requests and rejects invalid ones\n3. Test with various batch sizes (0, 1, 5, 6) to verify size constraints\n4. Test with malformed JSON to ensure proper error handling\n5. Verify the endpoint returns 400 for invalid requests and appropriate error messages",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement batch_judge_processing Function Using LangChain's abatch() Method",
        "description": "Create a function that processes multiple comparison requests in parallel using the existing individual prompt through LangChain's abatch() method to improve performance.",
        "details": "The implementation should follow these steps:\n\n1. Create a new function `batch_judge_processing` that accepts a list of CompareRequest objects.\n2. For each comparison in the list, prepare a batch input with the format:\n   ```python\n   {\n     \"question\": comparison.input,\n     \"answer_a\": comparison.response_a,\n     \"answer_b\": comparison.response_b\n   }\n   ```\n3. Use the existing \"laaj-prompt\" from LangSmith without modifications to maintain quality.\n4. Implement the parallel processing using LangChain's `chain.abatch(batch_inputs)` method:\n   ```python\n   async def batch_judge_processing(comparisons: List[CompareRequest]) -> List[CompareResult]:\n       batch_inputs = []\n       for comp in comparisons:\n           batch_inputs.append({\n               \"question\": comp.input,\n               \"answer_a\": comp.response_a,\n               \"answer_b\": comp.response_b\n           })\n       \n       try:\n           # Use the existing chain with abatch method\n           results = await chain.abatch(batch_inputs)\n           \n           # Process results to match expected output format\n           processed_results = []\n           for i, result in enumerate(results):\n               processed_results.append(CompareResult(\n                   comparison_id=comparisons[i].id,\n                   winner=result.get(\"winner\"),\n                   explanation=result.get(\"explanation\")\n               ))\n           \n           return processed_results\n       except Exception as e:\n           # Handle batch-level errors\n           logger.error(f\"Batch processing error: {str(e)}\")\n           raise\n   ```\n5. Implement individual error handling to ensure that if one comparison fails, it doesn't affect the others:\n   ```python\n   async def batch_judge_processing(comparisons: List[CompareRequest]) -> List[CompareResult]:\n       batch_inputs = []\n       for comp in comparisons:\n           batch_inputs.append({\n               \"question\": comp.input,\n               \"answer_a\": comp.response_a,\n               \"answer_b\": comp.response_b\n           })\n       \n       results = []\n       # Process each input individually but in parallel\n       async_results = await chain.abatch(batch_inputs)\n       \n       for i, (comp, async_result) in enumerate(zip(comparisons, async_results)):\n           try:\n               results.append(CompareResult(\n                   comparison_id=comp.id,\n                   winner=async_result.get(\"winner\"),\n                   explanation=async_result.get(\"explanation\")\n               ))\n           except Exception as e:\n               logger.error(f\"Error processing comparison {comp.id}: {str(e)}\")\n               # Add error result instead of failing the entire batch\n               results.append(CompareResult(\n                   comparison_id=comp.id,\n                   winner=None,\n                   explanation=f\"Error: {str(e)}\"\n               ))\n       \n       return results\n   ```\n6. Ensure the function maintains the original order of comparisons in the returned results.\n7. Add appropriate logging for monitoring performance and debugging.",
        "testStrategy": "1. Unit tests:\n   - Test the function with a mock LangChain chain to verify correct handling of inputs and outputs\n   - Test with various batch sizes (1, 3, 5) to ensure proper handling\n   - Test error scenarios by injecting failures in specific comparisons to verify isolation\n\n2. Integration tests:\n   - Test the function with actual LangChain integration using test prompts\n   - Verify that results match expected format and maintain correct ordering\n   - Measure performance improvements compared to sequential processing\n   - Test with edge cases (empty inputs, very long inputs)\n\n3. Performance testing:\n   - Benchmark the function with different batch sizes to determine optimal performance\n   - Compare execution time against sequential processing to quantify improvements\n   - Test under load to ensure stability with concurrent requests\n\n4. Error handling tests:\n   - Verify that errors in individual comparisons don't affect the entire batch\n   - Test with malformed inputs to ensure proper error reporting\n   - Verify logging is appropriate for debugging issues\n\n5. End-to-end tests:\n   - Test the function as part of the complete API flow\n   - Verify integration with the batch endpoint created in Task 1",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Develop Response Parser for Batch Results",
        "description": "Create a parsing system that processes results returned by the batch_judge_processing function and converts them to BatchComparisonResult format with appropriate mapping of fields and error handling.",
        "details": "The implementation should follow these steps:\n\n1. Create a new parser module that accepts the raw results list returned by abatch()\n2. Reuse the existing parsing logic from node_judge() function to process individual results:\n   - Detect if the result is structured JSON or natural text\n   - Apply the appropriate parsing strategy based on result type\n   - Extract the key fields (better_response, judge_reasoning) from each result\n3. For each successfully parsed result:\n   - Generate a unique UUID for the comparison result\n   - Create a BatchComparisonResult object with the parsed data\n   - Ensure the UUID is properly associated with each result\n4. Implement robust error handling:\n   - Catch and log parsing errors for individual results\n   - Continue processing other results even if one fails\n   - Include error information in the returned data structure\n5. Maintain the original order of comparisons in the returned results\n6. Return a structured response containing:\n   - Array of BatchComparisonResult objects\n   - Metadata about the batch processing (success count, error count)\n\nCode structure example:\n```python\ndef parse_batch_results(raw_results, original_requests):\n    \"\"\"\n    Parse results from batch_judge_processing into BatchComparisonResult format\n    \n    Args:\n        raw_results: List of results from abatch() call\n        original_requests: Original comparison requests to maintain order\n        \n    Returns:\n        List of BatchComparisonResult objects\n    \"\"\"\n    parsed_results = []\n    \n    for i, result in enumerate(raw_results):\n        try:\n            # Reuse existing parsing logic\n            parsed_data = parse_individual_result(result)\n            \n            # Create BatchComparisonResult with UUID\n            comparison_result = BatchComparisonResult(\n                id=str(uuid.uuid4()),\n                better_response=parsed_data.get(\"better_response\"),\n                judge_reasoning=parsed_data.get(\"judge_reasoning\"),\n                original_request=original_requests[i]\n            )\n            \n            parsed_results.append(comparison_result)\n        except Exception as e:\n            # Handle individual parsing errors\n            logger.error(f\"Error parsing result {i}: {str(e)}\")\n            parsed_results.append(BatchComparisonResult(\n                id=str(uuid.uuid4()),\n                error=str(e),\n                original_request=original_requests[i]\n            ))\n    \n    return parsed_results\n```",
        "testStrategy": "The testing strategy should include:\n\n1. Unit tests:\n   - Test parsing of various result formats (JSON, text, malformed)\n   - Verify correct extraction of fields (better_response, judge_reasoning)\n   - Test UUID generation to ensure uniqueness\n   - Verify error handling for individual result parsing failures\n   - Test preservation of original comparison order\n\n2. Integration tests:\n   - Test end-to-end flow from batch_judge_processing to parsed results\n   - Verify correct handling of mixed result types in a single batch\n   - Test with maximum batch size to ensure performance\n   - Verify correct association between original requests and parsed results\n\n3. Error handling tests:\n   - Test with completely malformed results\n   - Test with partially valid results (some good, some bad)\n   - Verify that errors in one result don't affect processing of others\n   - Test with empty result list\n\n4. Performance tests:\n   - Measure parsing time for different batch sizes\n   - Ensure memory usage remains reasonable for large batches\n\n5. Mock tests:\n   - Create mock abatch() results to test various edge cases\n   - Test with simulated network errors or timeouts",
        "status": "pending",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Integrate Batch Processing in Endpoint and Update Documentation",
        "description": "Connect all developed batch functions to the /api/v1/compare/batch endpoint and complete the full implementation, including error handling, metrics, and comprehensive documentation.",
        "details": "The implementation should follow these steps:\n\n1. Update the compare_models_batch() function in routers/compare.py:\n   - Connect to the batch_judge_processing() function developed in Task 2\n   - Pass the request.comparisons data to the batch processing function\n   - Implement proper error handling for various failure scenarios\n   - Add timeout handling to prevent long-running requests\n\n2. Implement response handling:\n   - Use the response parser from Task 3 to convert batch results into the proper format\n   - Structure the response as a BatchComparisonResponse object\n   - Include execution metrics (total execution time, successful comparisons count)\n   - Ensure all IDs and references are properly maintained\n\n3. Add comprehensive error handling:\n   - Handle individual comparison failures without failing the entire batch\n   - Implement appropriate HTTP status codes for different error scenarios\n   - Add detailed error messages to help with debugging\n   - Ensure timeout errors are gracefully handled\n\n4. Update documentation:\n   - Add detailed examples in CLAUDE.md showing batch usage patterns\n   - Document performance characteristics compared to individual processing\n   - Include curl/requests examples for testing the endpoint\n   - Document limitations (maximum 5 comparisons per batch)\n   - Add API reference documentation for the batch endpoint\n\n5. Implement execution metrics:\n   - Track and return total execution time\n   - Count and return successful vs. failed comparisons\n   - Add any relevant performance metrics",
        "testStrategy": "The testing strategy should include:\n\n1. Functional testing:\n   - Test the endpoint with different batch sizes (1, 3, 5)\n   - Verify correct handling of the maximum batch size limit\n   - Test with intentionally malformed requests to verify error handling\n   - Verify timeout handling works correctly for long-running comparisons\n\n2. Integration testing:\n   - Ensure compatibility with existing tests in requests.ipynb\n   - Verify that the endpoint correctly integrates with batch_judge_processing()\n   - Test the complete flow from request to response\n   - Validate that the response format matches the expected BatchComparisonResponse\n\n3. Error handling testing:\n   - Test scenarios where individual comparisons fail\n   - Verify that partial failures don't cause the entire batch to fail\n   - Test with invalid inputs to ensure proper validation\n   - Verify appropriate error messages are returned\n\n4. Performance testing:\n   - Compare execution times between batch and individual processing\n   - Verify that batch processing provides performance benefits\n   - Test with different batch sizes to measure scaling characteristics\n   - Ensure resource usage remains within acceptable limits\n\n5. Documentation verification:\n   - Verify all examples in the documentation work as described\n   - Test the curl/requests examples to ensure they function correctly\n   - Check that all limitations and constraints are accurately documented",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-10T21:50:09.672Z",
      "updated": "2025-08-24T16:05:36.457Z",
      "description": "Tasks for master context"
    }
  }
}